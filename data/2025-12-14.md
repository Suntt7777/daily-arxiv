<div id=toc></div>

# Table of Contents

- [math.OC](#math.OC) [Total: 10]
- [math.NA](#math.NA) [Total: 12]
- [cs.LG](#cs.LG) [Total: 52]
- [eess.IV](#eess.IV) [Total: 2]


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [1] [Quantitative results on a generalized viscosity approximation method](https://arxiv.org/abs/2512.09968)
*Paulo Firmino,Laurentiu Leustean*

Main category: math.OC

TL;DR: 该论文研究广义粘性逼近法在非线性空间中的渐近行为，应用证明挖掘方法获得定量结果


<details>
  <summary>Details</summary>
Motivation: 研究非线性空间中满足类预解条件的非扩张映射族的广义粘性逼近法的渐近行为，旨在获得定量收敛结果

Method: 应用证明挖掘方法，在W-双曲空间中分析渐近正则性，在CAT(0)空间中分析元稳定性收敛速率

Result: 获得了广义粘性逼近法在非线性空间中的定量渐近正则性结果和元稳定性收敛速率

Conclusion: 证明挖掘方法能有效分析非线性空间中广义粘性逼近法的渐近行为，为相关优化算法提供定量收敛保证

Abstract: In this paper, we study, in a nonlinear setting, the asymptotic behaviour of a generalized viscosity approximation method associated with a countable family of nonexpansive mappings satisfying resolvent-like conditions. We apply proof mining methods to obtain quantitative results on asymptotic regularity in W-hyperbolic spaces and rates of metastability in CAT(0) spaces.

</details>


### [2] [Fast projection onto the top-k-sum constraint](https://arxiv.org/abs/2512.10255)
*Jianting Pan,Ming Yan*

Main category: math.OC

TL;DR: 提出一种高效算法计算欧几里得投影到top-k-sum约束，避免排序操作，复杂度从O(nlogn)降低到O(n)


<details>
  <summary>Details</summary>
Motivation: 现有投影方法依赖排序操作，初始复杂度为O(nlogn)，限制了在高维设置下的可扩展性，需要更高效的算法

Method: 重新审视投影问题的KKT条件，引入松弛条件，通过几何解释将问题转化为寻找两个单调分段线性函数的交点，提出迭代算法直接搜索交点，完全避免排序过程

Result: 算法全局收敛并在有限迭代次数内达到精确解，数值实验显示算法显著优于现有方法，在广泛问题实例中表现出经验O(n)复杂度

Conclusion: 提出的算法通过避免排序操作，显著提高了top-k-sum约束投影的计算效率，为金融风险管理和矩阵优化问题提供了更高效的解决方案

Abstract: This paper develops an efficient algorithm for computing the Euclidean projection onto the top-k-sum constraint, a key operation in financial risk management and matrix optimization problems. Existing projection methods rely on sorting and therefore incur an initial O(nlogn) complexity, which limits their scalability in high-dimensional settings. To address this difficulty, we revisit the Karush-Kuhn-Tucker (KKT) conditions of the projection problem and introduce relaxed conditions that remain sufficient for characterizing the solution. These conditions lead to a simple geometric interpretation: finding the solutions is equivalent to locating the intersection of two monotone piecewise linear functions. Building on this insight, we propose an iterative and highly efficient algorithm that searches directly for the intersection point and completely avoids all sorting procedures. We prove that the algorithm converges globally and reaches the exact solution in a finite number of iterations. Extensive numerical experiments further demonstrate that the proposed algorithm substantially outperforms existing algorithms and exhibits empirical O(n) complexity across a broad range of problem instances.

</details>


### [3] [Optimality Deviation using the Koopman Operator](https://arxiv.org/abs/2512.10270)
*Yicheng Lin,Bingxian Wu,Nan Bai,Yunxiao Ren,Zhisheng Duan*

Main category: math.OC

TL;DR: 本文研究了在非线性系统数据驱动最优控制中使用Koopman算子时近似误差的影响，推导了最优控制器和价值函数偏差的显式上界，为提升数据驱动最优控制器设计的鲁棒性提供了量化基础。


<details>
  <summary>Details</summary>
Motivation: Koopman算子虽然能够通过提升状态空间简化非线性动力学的表示，但近似误差的存在不可避免地导致计算的最优控制器和价值函数出现偏差。需要量化这种偏差以改进数据驱动最优控制器的鲁棒性。

Method: 使用Koopman算子对非线性系统进行数据驱动最优控制，推导近似误差导致的最优控制器和价值函数偏差的显式上界，并通过数值示例进行验证。

Result: 获得了最优控制器和价值函数偏差的显式上界，这些上界刻画了近似误差的最坏情况影响，数值示例支持了理论发现。

Conclusion: 本文为数据驱动最优控制器设计提供了量化基础，通过推导的显式上界能够更好地理解和控制近似误差对最优控制性能的影响，从而提升控制系统的鲁棒性。

Abstract: This paper investigates the impact of approximation error in data-driven optimal control problem of nonlinear systems while using the Koopman operator. While the Koopman operator enables a simplified representation of nonlinear dynamics through a lifted state space, the presence of approximation error inevitably leads to deviations in the computed optimal controller and the resulting value function. We derive explicit upper bounds for these optimality deviations, which characterize the worst-case effect of approximation error. Supported by numerical examples, these theoretical findings provide a quantitative foundation for improving the robustness of data-driven optimal controller design.

</details>


### [4] [Primal-dual splitting for structured composite monotone inclusions with or without cocoercivity](https://arxiv.org/abs/2512.10366)
*Minh N. Dao,Hung M. Phan,Matthew K. Tam,Thang D. Truong*

Main category: math.OC

TL;DR: 提出了一种原始-对偶分裂算法，用于解决包含多个集值算子、有界线性算子组合以及非共轭单值算子的结构化复合单调包含问题，相比传统乘积空间方法降低了维度，允许更大步长范围。


<details>
  <summary>Details</summary>
Motivation: 现有算法在处理结构化复合单调包含问题时，通常需要将问题转化为两个极大单调算子的和，使用乘积空间技术，但这会增加维度且步长范围受限。需要一种更高效、更灵活的统一框架。

Method: 提出了一种原始-对偶分裂算法，能够处理包含多个集值算子、有界线性算子组合以及可能非共轭的单值算子的复杂结构。算法基于图结构设计，避免了传统乘积空间技术带来的维度增加问题。

Result: 该算法不仅统一了多种现有算法，还能生成新的图结构算法。相比乘积空间重构，降低了维度，允许不同的共轭或Lipschitz常数以及不同的预解参数，获得了更大的允许步长范围。

Conclusion: 提出的算法为结构化复合单调包含问题提供了一个高效、灵活的统一框架，通过去中心化融合lasso问题的数值实验验证了其实际应用价值。

Abstract: In this paper, we propose a primal-dual splitting algorithm for a broad class of structured composite monotone inclusions that involve finitely many set-valued operators, compositions of set-valued operators with bounded linear operators, and single-valued operators possibly without cocoercivity. The proposed algorithm is not only a unification for several contemporary algorithms but also a blueprint to generate new algorithms with graph-based structures. Our approach reduces dimensionality compared with the standard product space technique, which typically reformulates the original problem as the sum of two maximally monotone operators in order to apply splitting methods. It accommodates different cocoercive or Lipschitz constants as well as different resolvent parameters, and yields a larger allowable step-size range than in product space reformulations. We demonstrate the practicality of the approach by a numerical experiment on the decentralized fused lasso problem.

</details>


### [5] [Objective Coefficient Rounding and Almost Symmetries in Binary Programs](https://arxiv.org/abs/2512.10507)
*Dominik Kuzinowicz,Paweł Lichocki,Gioni Mexi,Marc E. Pfetsch,Sebastian Pokutta,Max Zimmer*

Main category: math.OC

TL;DR: 研究二进制程序中目标系数舍入与近似对称性的关系，通过减少有效位数加速求解并保证解质量


<details>
  <summary>Details</summary>
Motivation: 实证发现减少目标系数有效位数（舍入）通常使问题更容易求解，这可能是因为增加了对称性，而原始问题包含"近似对称性"

Method: 在容量设施选址问题、背包问题及其他多样化实例上，使用SCIP和CP-SAT求解器，通过舍入目标系数减少有效位数，研究对称性影响

Result: 对所有研究的问题类别，该方法都产生了更快的算法并保证了解的质量，对称性的影响取决于实例类型和求解器

Conclusion: 目标系数舍入能有效利用近似对称性加速二进制程序求解，同时提供原始目标值的近似解，是一种实用的优化技术

Abstract: This article investigates the interplay of rounding objective coefficients in binary programs and almost symmetries. Empirically, reducing the number of significant bits through rounding often leads to instances that are easier to solve. One reason can be that the amount of symmetries increases, which enables solvers to be more effective when they are exploited. This can signify that the original instance contains 'almost symmetries'. Furthermore, solving the rounded problems provides approximations to the original objective values. We empirically investigate these relations on instances of the capacitated facility location problem, the knapsack problem and a diverse collection of additional instances, using the solvers SCIP and CP-SAT. For all investigated problem classes, we show empirically that this yields faster algorithms with guaranteed solution quality. The influence of symmetry depends on the instance type and solver.

</details>


### [6] [Linear Quadratic Regulators: A New Look](https://arxiv.org/abs/2512.10641)
*CéEdric Join,Emmanuel Delaveau,Michel Fliess*

Main category: math.OC

TL;DR: 该论文提出了一种基于代数系统理论和模型自由控制的线性时不变系统控制方法，将系统模块的自由性与最优控制相结合，通过平坦输出实现开环控制，并利用智能控制器处理模型失配和扰动。


<details>
  <summary>Details</summary>
Motivation: 传统线性时不变控制系统的Kalman可控性在代数框架下可表示为系统模块的自由性，但如何将这种代数特性与最优控制（如线性二次调节器）相结合，并有效处理模型失配和扰动问题，是研究的动机。

Method: 1. 将线性时不变系统视为实数域上线性微分算子环的有限生成模；2. 利用平坦输出（自由模的基）通过欧拉-拉格朗日方程实现开环控制；3. 处理两点边值问题以获得最优时间范围、参数设计和轨迹；4. 采用模型自由控制方法设计智能控制器以闭环控制。

Result: 该方法能够处理包含控制变量的两点边值问题，得到最优时间范围、最优参数设计和最优静止到静止轨迹，并通过模型自由控制器有效应对模型失配和扰动。

Conclusion: 通过结合代数系统理论中的模块自由性与最优控制，并利用模型自由控制实现闭环，该方法为线性时不变系统提供了一种系统化的控制策略，能够同时处理最优轨迹设计和鲁棒性问题。

Abstract: Linear time-invariant control systems can be considered as finitely generated modules over the commutative principal ideal ring $\mathbb{R}[\frac{d}{dt}]$ of linear differential operators with respect to the time derivative. The Kalman controllability in this algebraic language is translated as the freeness of the system module. Linear quadratic regulators rely on quadratic Lagrangians, or cost functions. Any flat output, i.e., any basis of the corresponding free module leads to an open-loop control strategy via an Euler-Lagrange equation, which becomes here a linear ordinary differential equation with constant coefficients. In this approach, the two-point boundary value problem, including the control variables, becomes tractable. It yields notions of optimal time horizon, optimal parameter design and optimal rest-to-rest trajectories. The loop is closed via an intelligent controller derived from model-free control, which is known to exhibit excellent performance concerning model mismatches and disturbances.

</details>


### [7] [Strong Global Convergence of the Consensus-Based Optimization Algorithm](https://arxiv.org/abs/2512.10654)
*Sabrina Bonandin,Konstantin Riedl,Sara Veneruso*

Main category: math.OC

TL;DR: 本文证明了基于共识的优化算法在数值时间离散化下的强均方收敛性，为各向同性和各向异性扩散提供了明确的收敛速率和超参数选择条件。


<details>
  <summary>Details</summary>
Motivation: CBO算法虽然在实际应用中表现出色且易于理论分析，但其数值时间离散化版本到全局最小值的收敛性尚未得到严格证明，特别是对于不满足全局Lipschitz条件的系数情况。

Method: 通过将时间离散算法解释为连续时间水平的随机微分方程系统，结合传统SDE数值方法的有限时间收敛理论，并采用Sznitman经典论证的推广来处理非全局Lipschitz系数问题。

Result: 证明了CBO算法对一类丰富的目标函数具有强均方收敛性，给出了关于时间步长Δt和粒子数N的显式收敛速率，并为各向同性和各向异性扩散提供了超参数选择条件。

Conclusion: CBO算法的数值时间离散版本能够收敛到全局最小值，这为算法的实际应用提供了坚实的理论基础，特别是在处理非凸非光滑函数优化问题时。

Abstract: Consensus-based optimization (CBO) is a multi-agent metaheuristic derivative-free optimization algorithm that has proven to be capable of globally minimizing nonconvex nonsmooth functions across a diverse range of applications while being amenable to theoretical analysis. The method leverages an interplay between exploration of the energy landscape of the objective function through a system of interacting particles subject to stochasticity and exploitation of the particles' positions through the computation of a global consensus about the location of the minimizer based on the Laplace principle. In this paper, we prove strong mean square convergence of the practical numerical time-discrete CBO algorithm to the global minimizer for a rich class of objective functions. For CBO with both isotropic and anisotropic diffusion, our convergence result features conditions on the choice of the hyperparameters as well as explicit rates of convergence in the time discretization step size $Δt$ and the number of particles $N$. By interpreting the time-discrete algorithm at the continuous-time level through a system of stochastic differential equations (SDEs), our proof strategy combines traditional finite-time convergence theory for numerical methods applied to SDEs with careful considerations due to the fact that the CBO coefficients do not satisfy a global Lipschitz condition. To accomodate the latter, we adopt a recently proposed generalization of Sznitman's classical argument, which allows to discard an event of small probability, controllable through fine moment estimates for the particle systems.

</details>


### [8] [On the Convergence Analysis of an Inexact Preconditioned Stochastic Model-Based Algorithm](https://arxiv.org/abs/2512.10826)
*Chenglong Bao,Yancheng Yuan,Shulan Zhu*

Main category: math.OC

TL;DR: 提出一种结合预条件技术的非精确随机模型优化算法，用于解决随机复合优化问题，建立了收敛保证和收敛速率


<details>
  <summary>Details</summary>
Motivation: 针对随机复合优化问题，现有固定度量的随机模型算法存在局限性，需要扩展到预条件和非精确变体，以处理更广泛的优化场景

Method: 提出一个统一框架，将固定度量的随机模型算法扩展到预条件和非精确变体，在弱凸和凸设置下建立收敛保证，利用Moreau包络的梯度作为收敛度量

Result: 在局部Lipschitz条件下推导了非渐近和渐近收敛速率，在目标函数满足二次增长条件时获得了到最优解集距离的收敛速率，数值实验验证了理论结果

Conclusion: 该算法框架统一并扩展了现有随机模型优化方法，在较弱的假设条件下提供了收敛保证，为随机复合优化问题提供了有效的解决方案

Abstract: This paper focuses on investigating an inexact stochastic model-based optimization algorithm that integrates preconditioning techniques for solving stochastic composite optimization problems. The proposed framework unifies and extends the fixed-metric stochastic model-based algorithm to its preconditioned and inexact variants. Convergence guarantees are established under mild assumptions for both weakly convex and convex settings, without requiring smoothness or global Lipschitz continuity of the objective function. By assuming a local Lipschitz condition, we derive nonasymptotic and asymptotic convergence rates measured by the gradient of the Moreau envelope. Furthermore, convergence rates in terms of the distance to the optimal solution set are obtained under an additional quadratic growth condition on the objective function. Numerical experiment results demonstrate the theoretical findings for the proposed algorithm.

</details>


### [9] [Indirect methods in optimal control on Banach spaces](https://arxiv.org/abs/2512.10831)
*Roman Chertovskih,Nikolay Pogodaev,Maxim Staritsyn,A. Pedro Aguiar*

Main category: math.OC

TL;DR: 本文提出了一种基于精确成本增量公式和终端成本有限差分探测的间接下降方法，用于解决Banach空间中非线性常微分方程控制的最优控制问题，相比传统基于庞特里亚金极大值原理的方法具有更好的单调收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统基于庞特里亚金极大值原理的间接下降方法对局部凸性敏感且缺乏单调收敛性，需要开发更稳定、收敛性更好的替代方法来解决非线性常微分方程控制的最优控制问题。

Method: 开发了一种基于精确成本增量公式和终端成本有限差分探测的间接下降方法，该方法不依赖于庞特里亚金极大值原理，而是通过精确计算成本增量来指导优化过程。

Result: 在Amari型神经场控制问题的数值分析中，该方法表现出稳定的单调收敛性，相比传统方法具有更好的数值稳定性。

Conclusion: 提出的基于精确成本增量公式和有限差分探测的间接下降方法为非线性常微分方程控制的最优控制问题提供了更稳定、收敛性更好的数值求解方案。

Abstract: This work focuses on indirect descent methods for optimal control problems governed by nonlinear ordinary differential equations in Banach spaces, viewed as abstract models of distributed dynamics. As a reference line, we revisit the classical schemes, rooted in Pontryagin's maximum principle, and highlight their sensitivity to local convexity and lack of monotone convergence. We then develop an alternative method based on exact cost-increment formulas and finite-difference probes of the terminal cost. We show that our method exhibits stable monotone convergence in numerical analysis of an Amari-type neural field control problem.

</details>


### [10] [Distributionally Robust Regret Optimal Control Under Moment-Based Ambiguity Sets](https://arxiv.org/abs/2512.10906)
*Feras Al Taha,Eilyan Bitar*

Main category: math.OC

TL;DR: 该论文研究有限时域线性二次随机控制问题，在噪声分布未知但属于均值协方差模糊集的情况下，设计因果仿射控制策略以最小化最坏情况期望遗憾，并提出可扩展的对偶投影次梯度方法求解。


<details>
  <summary>Details</summary>
Motivation: 实际控制问题中噪声分布通常未知，传统方法假设精确分布可能导致性能下降。需要设计对分布模糊具有鲁棒性的控制策略，同时保证计算可行性。

Method: 1) 建立基于均值协方差模糊集的最坏情况期望遗憾最小化问题；2) 将极小极大最优控制问题转化为可处理的凸规划问题；3) 提出对偶投影次梯度方法进行高效求解。

Result: 1) 证明了极小极大问题可等价转化为正则化版本的标称线性二次随机控制问题；2) 提出的对偶投影次梯度方法能够以任意精度计算最优控制器；3) 数值实验表明该方法优于现有数据驱动和分布鲁棒控制方法。

Conclusion: 该研究为分布模糊下的随机控制问题提供了理论框架和高效算法，将极小极大遗憾最小化问题转化为可处理的凸优化问题，并通过可扩展算法实现实际应用。

Abstract: In this paper, we consider a class of finite-horizon, linear-quadratic stochastic control problems, where the probability distribution governing the noise process is unknown but assumed to belong to an ambiguity set consisting of all distributions whose mean and covariance lie within norm balls centered at given nominal values. To address the distributional ambiguity, we explore the design of causal affine control policies to minimize the worst-case expected regret over all distributions in the given ambiguity set. The resulting minimax optimal control problem is shown to admit an equivalent reformulation as a tractable convex program that corresponds to a regularized version of the nominal linear-quadratic stochastic control problem. While this convex program can be recast as a semidefinite program, semidefinite programs are typically solved using primal-dual interior point methods that scale poorly with the problem size in practice. To address this limitation, we propose a scalable dual projected subgradient method to compute optimal controllers to an arbitrary accuracy. Numerical experiments are presented to benchmark the proposed method against state-of-the-art data-driven and distributionally robust control design approaches.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [11] [Hybrid Finite Element and Least Squares Support Vector Regression Method for solving Partial Differential Equations with Legendre Polynomial Kernels](https://arxiv.org/abs/2512.09967)
*Maryam Babaei,Peter Rucz,Manfred Kaltenbacher,Stefan Schoder*

Main category: math.NA

TL;DR: 提出了一种结合有限元法和最小二乘支持向量回归的混合计算方法，用于求解偏微分方程，通过局部核细化实现超分辨率，显著提高精度。


<details>
  <summary>Details</summary>
Motivation: 传统有限元法虽然能提供节点解，但无法获得节点间的解析解。需要一种方法能在保持有限元节点值一致性的同时，实现高精度插值和高分辨率解。

Method: 将有限元法与最小二乘支持向量回归结合，使用高阶Legendre多项式核函数，通过单元增强（超分辨率）技术，在保持单元边界节点值一致性的同时获得闭式解析解。

Result: 混合方法相比基础有限元解精度显著提高，与使用相同多项式基函数阶数的独立有限元结果精度相当。对四个椭圆边值问题的收敛性研究证明了方法的有效性、准确性和可靠性。

Conclusion: 该方法可作为即插即用方法用于超分辨率低阶数值求解器，也可用于处理昂贵或分辨率不足的实验数据，无需额外实现开销，支持并行计算。

Abstract: A hybrid computational approach that integrates the finite element method (FEM) with least squares support vector regression (LSSVR) is introduced to solve partial differential equations. The method combines FEM's ability to provide the nodal solutions and LSSVR with higher-order Legendre polynomial kernels to deliver a closed-form analytical solution for interpolation between the nodes. The hybrid approach implements element-wise enhancement (super-resolution) of a given numerical solution, resulting in high resolution accuracy, while maintaining consistency with FEM nodal values at element boundaries. It can adapt any low-order FEM code to obtain high-order resolution by leveraging localized kernel refinement and parallel computation without additional implementation overhead. Therefore, effective inference/post-processing of the obtained super-resolved solution is possible. Evaluation results show that the hybrid FEM-LSSVR approach can achieve significantly higher accuracy compared to the base FEM solution. Comparable accuracy is a achieved when comparing the hybrid solution with a standalone FEM result with the same polynomial basis function order. The convergence studies were conducted for four elliptic boundary value problems to demonstrate the method's ability, accuracy, and reliability. Finally, the algorithm can be directly used as a plug-and-play method for super-resolving low-order numerical solvers and for super-resolution of expensive/under-resolved experimental data.

</details>


### [12] [Efficient Boys function evaluation using minimax approximation](https://arxiv.org/abs/2512.10059)
*Rasmus Vikhamar-Sandberg,Michal Repisky*

Main category: math.NA

TL;DR: 提出一种针对GPU架构优化的Boys函数高效评估算法，结合有理极小极大逼近和递推关系，避免查找表和不规则内存访问，实现高吞吐量计算。


<details>
  <summary>Details</summary>
Motivation: 针对现代计算架构（特别是GPU）的特点，需要开发能够充分利用高吞吐量、避免数据移动成本的Boys函数评估方法，以提升计算效率。

Method: 将非负实轴划分为三个区域：A和B区域使用有理极小极大逼近，C区域使用渐近逼近；结合向上和向下递推关系；通过有理Remez算法生成逼近系数。

Result: 算法实现了目标最大绝对误差ε_tol=5×10^{-14}的精度，为Boys函数F_0到F_32提供了相应的逼近区域和系数。

Conclusion: 该方法特别适合具有高最大吞吐量和低延迟的硬件架构，避免了查找表和不规则内存访问，为GPU上的Boys函数计算提供了高效解决方案。

Abstract: We present an algorithm for efficient evaluation of Boys functions $F_0,\dots,F_{k_\mathrm{max}}$ tailored to modern computing architectures, in particular graphical processing units (GPUs), where maximum throughput is high and data movement is costly. The method combines rational minimax approximations with upward and downward recurrence relations. The non-negative real axis is partitioned into three regions, $[0,\infty\rangle = A\cup B\cup C$, where regions $A$ and $B$ are treated using rational minimax approximations and region $C$ by an asymptotic approximation. This formulation avoids lookup tables and irregular memory access, making it well suited hardware with high maximum throughput and low latency. The rational minimax coefficients are generated using the rational Remez algorithm. For a target maximum absolute error of $\varepsilon_\mathrm{tol} = 5\cdot10^{-14}$, the corresponding approximation regions and coefficients for Boys functions $F_0,\dots,F_{32}$ are provided in the appendix.

</details>


### [13] [Metric-driven numerical methods](https://arxiv.org/abs/2512.10083)
*Patrick Henning,Laura Huynh,Daniel Peterseim*

Main category: math.NA

TL;DR: 该论文提出了一种基于度量的数值方法，用于求解多尺度偏微分方程，特别是约束极小化问题和特征值问题。通过黎曼梯度技术和Sobolev梯度概念，该方法能加速收敛并生成具有增强性质的逼近空间。


<details>
  <summary>Details</summary>
Motivation: 解决多尺度偏微分方程中的约束极小化问题和特征值问题，特别是在低正则性或具有异质多尺度特征的情况下，传统方法面临收敛困难和逼近精度不足的挑战。

Method: 引入基于度量的数值方法，利用黎曼梯度技术，通过不同度量（Sobolev梯度）表示梯度来加速收敛。该方法不仅产生特定的度量驱动迭代格式，还能诱导出具有增强性质的逼近空间。

Result: 该方法能够从新的视角推导出基于局部正交分解（LOD）的著名多尺度空间类，并成功应用于模拟自旋轨道耦合玻色-爱因斯坦凝聚体的基态。

Conclusion: 度量驱动的数值方法为解决多尺度偏微分方程提供了一种强大的工具，特别是在处理约束极小化、特征值问题和低正则性情况时，通过合适的度量选择能够显著改善收敛性和逼近精度。

Abstract: In this paper, we explore the concept of metric-driven numerical methods as a powerful tool for solving various types of multiscale partial differential equations. Our focus is on computing constrained minimizers of functionals - or, equivalently, by considering the associated Euler-Lagrange equations - the solution of a class of eigenvalue problems that may involve nonlinearities in the eigenfunctions. We introduce metric-driven methods for such problems via Riemannian gradient techniques, leveraging the idea that gradients can be represented in different metrics (so-called Sobolev gradients) to accelerate convergence. We show that the choice of metric not only leads to specific metric-driven iterative schemes, but also induces approximation spaces with enhanced properties, particularly in low-regularity regimes or when the solution exhibits heterogeneous multiscale features. In fact, we recover a well-known class of multiscale spaces based on the Localized Orthogonal Decomposition (LOD), now derived from a new perspective. Alongside a discussion of the metric-driven approach for a model problem, we also demonstrate its application to simulating the ground states of spin-orbit-coupled Bose-Einstein condensates.

</details>


### [14] [Numerical approximation of the first $p$-Laplace eigenpair](https://arxiv.org/abs/2512.10122)
*Hannah Potgieter,Razvan C. Fetecau,Steven J. Ruuth*

Main category: math.NA

TL;DR: 该论文提出了一种数值方法，用于近似计算p-拉普拉斯算子在欧几里得域和曲面域上的第一Dirichlet特征对，特别关注大p值情况，并探讨p→∞极限与域几何结构的关系。


<details>
  <summary>Details</summary>
Motivation: 研究p-拉普拉斯算子的第一Dirichlet特征对，特别是当p值很大时，数值计算面临显著挑战。需要开发稳定的数值方法来处理大p值情况，并探索p→∞极限与域几何性质的联系。

Method: 提出了一种曲面有限元数值方案，结合牛顿反幂迭代法和新的域重缩放策略，实现了对大p值的稳定计算。该方法适用于1D、平面域和嵌入ℝ³的曲面。

Result: 数值实验证明该方法具有准确性和鲁棒性，能够稳定计算大p值下的特征对，并显示出向p→∞极限行为的收敛趋势。

Conclusion: 所提出的数值方法成功解决了大p值p-拉普拉斯算子特征值计算的稳定性问题，揭示了p→∞极限与域几何结构的内在联系，为相关研究提供了有效的计算工具。

Abstract: We approximate the first Dirichlet eigenpair of the $p$-Laplace operator for $2 \leq p < \infty$ on both Euclidean and surface domains. We emphasize large $p$ values and discuss how the $p \to \infty$ limit connects to the underlying geometry of our domain. Working with large $p$ values introduces significant numerical challenges. We present a surface finite element numerical scheme that combines a Newton inverse-power iteration with a new domain rescaling strategy, which enables stable computations for large $p$. Numerical experiments in $1$D, planar domains, and surfaces embedded in $\mathbb{R}^3$ demonstrate the accuracy and robustness of our approach and show convergence towards the $p \to \infty$ limiting behavior.

</details>


### [15] [A robust fully-mixed finite element method with skew-symmetry penalization for low-frequency poroelasticity](https://arxiv.org/abs/2512.10192)
*Stefano Bonetti,Michele Botti,Patrick Vega*

Main category: math.NA

TL;DR: 提出并分析了一种用于低频动态多孔弹性问题的全混合有限元方法，将问题表示为四场一阶双曲系统，通过罚函数施加应力对称约束，使用稳定混合元进行空间离散和隐式时间推进。


<details>
  <summary>Details</summary>
Motivation: 针对低频动态多孔弹性问题，需要开发稳定且鲁棒的数值方法，能够处理模型参数退化情况，并准确模拟多孔材料中的波传播现象。

Method: 将问题表示为四场一阶双曲系统，通过罚函数方法施加应力对称约束；采用稳定混合有限元进行空间离散，结合隐式时间推进方案；稳定性分析考虑了模型参数退化情况。

Result: 数值测试验证了方法的收敛性和鲁棒性，评估了该方法在多孔材料波传播模拟中的性能表现，稳定性分析对有意义模型参数退化情况具有完全鲁棒性。

Conclusion: 该方法为低频动态多孔弹性问题提供了一种稳定、鲁棒的数值解决方案，能够有效模拟多孔材料中的波传播现象，对工程应用具有实际价值。

Abstract: In this work, we present and analyze a fully-mixed finite element scheme for the dynamic poroelasticity problem in the low-frequency regime. We write the problem as a four-field, first-order, hyperbolic system of equations where the symmetry constraint on the stress field is imposed via penalization. This strategy is equivalent to adding a perturbation to the saddle point system arising when the stress symmetry is weakly-imposed. The coupling of solid and fluid phases is discretized by means of stable mixed elements in space and implicit time advancing schemes. The presented stability analysis is fully robust with respect to meaningful cases of degenerate model parameters. Numerical tests validate the convergence and robustness and assess the performances of the method for the simulation of wave propagation phenomena in porous materials.

</details>


### [16] [Variational-hemivariational inequalities: A brief survey on mathematical theory and numerical analysis](https://arxiv.org/abs/2512.10204)
*Weimin Han*

Main category: math.NA

TL;DR: 该综述论文简要介绍了变分-半变分不等式的适定性和数值分析结果，包括理论框架、存在唯一性证明，并通过力学问题对比展示了这类不等式的特点。


<details>
  <summary>Details</summary>
Motivation: 变分-半变分不等式是变分不等式的自然扩展，能够处理物理科学和工程中涉及非光滑甚至集值关系的应用问题。近年来，该领域在建模、适定性分析、数值方法和算法开发方面取得了显著进展，需要对这些成果进行系统总结。

Method: 论文采用综述方法，首先为抽象静态变分-半变分不等式家族提供理论结果，解释存在唯一性的可访问证明思路。通过引入三个力学问题（分别导致变分方程、变分不等式和变分-半变分不等式）进行比较分析，突出变分-半变分不等式的特点。还讨论了混合变分-半变分不等式及其在流体力学中的应用，以及非静态和历史相关类型的数值解结果。

Result: 论文系统总结了变分-半变分不等式的适定性和数值分析研究成果，提供了抽象理论框架，展示了存在唯一性证明的主要思路，并通过具体力学问题对比阐明了这类不等式的独特特征和应用价值。

Conclusion: 变分-半变分不等式是一个充满挑战且应用广泛的数学领域，该综述论文为理解其理论基础、适定性分析和数值方法提供了全面概述，有助于推动该领域的进一步研究和发展。

Abstract: Variational-hemivariational inequalities are an area full of interesting and challenging mathematical problems. The area can be viewed as a natural extension of that of variational inequalities. Variational-hemivariational inequalities are valuable for application problems from physical sciences and engineering that involve non-smooth and even set-valued relations, monotone or non-monotone, among physical quantities. In the recent years, there has been substantial growth of research interest in modeling, well-posedness analysis, development of numerical methods and numerical algorithms of variational-hemivariational inequalities. This survey paper is devoted to a brief account of well-posedness and numerical analysis results for variational-hemivariational inequalities. The theoretical results are presented for a family of abstract stationary variational-hemivariational inequalities and the main idea is explained for an accessible proof of existence and uniqueness. To better appreciate the distinguished feature of variational-hemivariational inequalities, for comparison, three mechanical problems are introduced leading to a variational equation, a variational inequality, and a variational-hemivariational inequality, respectively. The paper also comments on mixed variational-hemivariational inequalities, with examples from applications in fluid mechanics, and on results concerning the numerical solution of other types (nonstationary, history dependent) of variational-hemivariational inequalities.

</details>


### [17] [Convergence analysis of contrast source inversion type methods for acoustic inverse medium scattering problems](https://arxiv.org/abs/2512.10260)
*Qiao Hu,Bo Zhang,Haiwen Zhang*

Main category: math.NA

TL;DR: 本文提出了两种新的迭代正则化CSI方法（IRCSI和IRSOM），首次证明了在固定频率下求解非线性逆散射问题的迭代方法的全局收敛性。


<details>
  <summary>Details</summary>
Motivation: CSI型方法（对比源反演和子空间优化方法）是求解逆介质散射问题的有效流行方法，但其严格的收敛性一直是一个未解决的问题。

Method: 提出了两种迭代正则化CSI型方法：IRCSI和IRSOM，采用新颖的ℓ1近端项作为迭代正则化项，计算复杂度与原始CSI和SOM方法相似。

Result: 证明了这两种方法在原始目标函数的自然弱条件下的全局收敛性，这是首次获得固定频率非线性逆散射问题迭代方法的收敛结果。

Conclusion: 通过数值实验验证了两种IRCSI型算法的收敛性和性能，为解决逆散射问题的收敛性难题提供了理论保证。

Abstract: The contrast source inversion (CSI) method and the subspace-based optimization method (SOM) are first proposed in 1997 and 2009, respectively, and subsequently modified. The two methods and their variants share several properties and thus are called the CSI-type methods. The CSI-type methods are efficient and popular methods for solving inverse medium scattering problems, but their rigorous convergence remains an open problem. In this paper, we propose two iteratively regularized CSI-type (IRCSI-type) methods with a novel $\ell_1$ proximal term as the iteratively regularized term: the iteratively regularized CSI (IRCSI) method and the iteratively regularized SOM (IRSOM) method, which have a similar computation complexity to the original CSI and SOM methods, respectively, and prove their global convergence under natural and weak conditions on the original objective function. To the best of our knowledge, this is the first convergence result for iterative methods of solving nonlinear inverse scattering problems with a fixed frequency. The convergence and performance of the two IRCSI-type algorithms are illustrated by numerical experiments.

</details>


### [18] [Matrix approach to the fractional calculus](https://arxiv.org/abs/2512.10330)
*V. N. Kolokoltsov,E. L. Shishkina*

Main category: math.NA

TL;DR: 本文提出了一种基于矩阵方法的分数阶导数和积分新构造，通过函数相关的微分算子生成半群，离散化得到矩阵近似，利用Balakrishnan表示法近似分数阶算子，并推导了收敛率，同时给出了双带矩阵任意幂的显式计算式。


<details>
  <summary>Details</summary>
Motivation: 为分数阶微分和积分方程提供一种强大的解析和数值计算工具，通过矩阵方法构造分数阶算子，利用半群理论实现高效近似。

Method: 1. 从生成半群的函数相关微分算子出发；2. 离散化算子得到矩阵近似；3. 应用Balakrishnan的基于半群的算子分数幂表示法；4. 利用半群范数和算子与矩阵近似差的范数估计推导收敛率；5. 推导双带矩阵任意幂的显式计算式。

Result: 1. 获得了算子分数幂与对应矩阵算子分数幂近似的收敛率；2. 得到了计算双带矩阵任意幂的显式公式；3. 为分数阶微分和积分方程的数值解提供了有效工具。

Conclusion: 提出的基于矩阵方法的分数阶导数和积分构造是一种强大的分析工具，通过半群理论和矩阵近似实现了分数阶算子的高效数值计算，为分数阶微分积分方程的求解提供了新途径。

Abstract: In this paper, we introduce the new construction of fractional derivatives and integrals with respect to a function, based on a matrix approach. We believe that this is a powerful tool in both analytical and numerical calculations. We begin with the differential operator with respect to a function that generates a semigroup. By discretizing this operator, we obtain a matrix approximation. Importantly, this discretization provides not only an approximating operator but also an approximating semigroup. This point motivates our approach, as we then apply Balakrishnan's representations of fractional powers of operators, which are based on semigroups. Using estimates of the semigroup norm and the norm of the difference between the operator and its matrix approximation, we derive the convergence rate for the approximation of the fractional power of operators with the fractional power of correspondings matrix operators. In addition, an explicit formula for calculating an arbitrary power of a two-band matrix is obtained, which is indispensable in the numerical solution of fractional differential and integral equations.

</details>


### [19] [Second order reduced model via incremental projection for Navier Stokes](https://arxiv.org/abs/2512.10473)
*Mejdi Azaïez,Yayu Guo,Carlos Núñez Fernández,Samuele Rubino,Chuanju Xu*

Main category: math.NA

TL;DR: 该论文提出了一种基于增量投影方案的Stokes方程降阶建模方法，采用POD技术构建降阶模型，实现了速度和压力的显式计算，保持了二阶时间精度。


<details>
  <summary>Details</summary>
Motivation: 不可压缩流动的数值模拟面临速度和压力紧密耦合的挑战，投影方法通过解耦这些变量为大规模计算提供了有效解决方案。本文旨在开发适用于Stokes方程的降阶建模方法，以提高计算效率。

Method: 采用增量投影方案处理Stokes方程，结合BDF2时间离散和有限元空间离散。使用本征正交分解(POD)构建降阶模型，实现速度和压力的显式计算。提供了详细的稳定性分析和误差估计。

Result: 理论分析表明该方法具有二阶时间收敛性。数值实验验证了理论结果，并证明了计算效率的提升。降阶模型在保持精度的同时显著减少了计算成本。

Conclusion: 提出的基于增量投影方案的POD降阶建模方法为Stokes方程提供了一种高效、准确的数值求解方案，适用于大规模不可压缩流动模拟。

Abstract: The numerical simulation of incompressible flows is challenging due to the tight coupling of velocity and pressure. Projection methods offer an effective solution by decoupling these variables, making them suitable for large-scale computations. This work focuses on reduced-order modeling using incremental projection schemes for the Stokes equations. We present both semi-discrete and fully discrete formulations, employing BDF2 in time and finite elements in space. A proper orthogonal decomposition (POD) approach is adopted to construct a reduced-order model for the Stokes problem. The method enables explicit computation of reduced velocity and pressure while preserving accuracy. We provide a detailed stability analysis and derive error estimates, showing second-order convergence in time. Numerical experiments are conducted to validate the theoretical results and demonstrate computational efficiency.

</details>


### [20] [Analysis of discrete energy-decay preserving schemes for Maxwell's equations in Cole-Cole dispersive medium](https://arxiv.org/abs/2512.10560)
*Guoyu Zhang,Ziming Dong,Baoli Yin,Yang Liu,Hong Li*

Main category: math.NA

TL;DR: 本文为Cole-Cole色散介质中的麦克斯韦方程设计和分析了能量衰减保持数值格式，提出了新型θ-格式，在特定θ范围内保持离散能量耗散特性，并验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 研究Cole-Cole色散介质中麦克斯韦方程的数值解法，需要设计能够保持物理系统能量衰减特性的数值格式，以确保数值模拟的物理保真度和长期稳定性。

Method: 首先建立了Cole-Cole模型的连续能量衰减定律，然后提出了一种新颖的θ-格式进行时间离散化，该格式在θ∈[α/2, 1/2]条件下严格保持离散能量耗散特性。

Result: 理论证明θ-格式在θ≠0.5时具有一阶时间收敛率，在θ=0.5时具有二阶收敛率。数值实验验证了理论结果，SFTR-θ格式在保持单调能量衰减方面优于二阶分数阶向后差分公式，特别是在长时间模拟中表现出更好的鲁棒性和物理保真度。

Conclusion: 提出的SFTR-θ格式为Cole-Cole色散介质中的麦克斯韦方程提供了一种有效的数值解法，能够保持能量衰减特性，具有理论保证的收敛性和优越的长期稳定性，适用于需要物理保真度的数值模拟。

Abstract: This work investigates the design and analysis of energy-decay preserving numerical schemes for Maxwell's equations in a Cole-Cole (C-C) dispersive medium. A continuous energy-decay law is first established for the C-C model through a modified energy functional. Subsequently, a novel \(θ\)-scheme is proposed for temporal discretization, which is rigorously proven to preserve a discrete energy dissipation property under the condition \(θ\in [\fracα{2}, \frac{1}{2}]\). The temporal convergence rate of the scheme is shown to be first-order for \(θ\neq 0.5\) and second-order for \(θ= 0.5\). Extensive numerical experiments validate the theoretical findings, including convergence tests and energy-decay comparisons. The proposed SFTR-\(θ\) scheme demonstrates superior performance in maintaining monotonic energy decay compared to an alternative 2nd-order fractional backward difference formula, particularly in long-time simulations, highlighting its robustness and physical fidelity.

</details>


### [21] [Dynamically consistent finite volume scheme for a bimonomeric simplified model with inflammation processes for Alzheimer's disease](https://arxiv.org/abs/2512.10716)
*Juan Barajas-Calonge,Mauricio A. Sepulveda Cortes,Nicolas Torres,Luis Miguel Villada*

Main category: math.NA

TL;DR: 本文开发了用于阿尔茨海默病进展模型的有限体积数值方案，证明了离散解的存在性和收敛性，并通过数值实验验证了方案性能。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病进展模型包含复杂的生物相互作用，需要开发数值稳定的有限体积方案来处理这个对流-扩散-反应系统，确保解的非负性和有界性。

Method: 开发了有限体积方案处理四偏微分方程和一常微分方程系统，采用半隐式策略离散反应项，与空间齐次模型的非标准离散化一致，证明了离散解的存在性和收敛性。

Result: 建立了有限体积方案，证明了离散解的存在性、非负性和先验界，方案收敛到模型的容许弱解，数值实验验证了模型和方案的行为。

Conclusion: 所开发的有限体积方案对阿尔茨海默病进展模型具有动态一致性，能够有效处理复杂的生物相互作用系统，为疾病建模提供了可靠的数值工具。

Abstract: A model of progression of Alzheimer's disease (AD) incorporating the interactions of A$β$-monomers, oligomers, microglial cells and interleukins with neurons is considered. The resulting convection-diffusion-reaction system consists of four partial differential equations (PDEs) and one ordinary differential equation (ODE). We develop a finite volume (FV) scheme for this system, together with non-negativity and a priori bounds for the discrete solution, so that we establish the existence of a discrete solution to the FV scheme. It is shown that the scheme converges to an admissible weak solution of the model. The reaction terms of the system are discretized using a semi-implicit strategy that coincides with a nonstandard discretization of the spatially homogeneous (SH) model. This construction enables us to prove that the FV scheme is dynamically consistent with respect to the spatially homogeneous version of the model. Finally, numerical experiments are presented to illustrate the model and to assess the behavior of the FV scheme.

</details>


### [22] [A Stabilized Finite Element Method for Morpho-Visco-Poroelastic Model](https://arxiv.org/abs/2512.10718)
*Sabia Asghar,Duncan den Bakker,Etelvina Javierre,Qiyao Peng,Fred J. Vermolen*

Main category: math.NA

TL;DR: 提出结合弹性、粘性、多孔效应与微结构变化引起生长/收缩的数学模型，分析连续和半离散版本的平衡稳定性，描述数值解单调性条件及稳定化方法以避免伪振荡，通过计算机模拟验证稳定化效果。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于建立能够描述组织或肿瘤生长以及皮肤收缩等生物过程中弹性、粘性、多孔效应与微结构变化引起生长/收缩的综合数学模型。这些现象在生物医学领域具有重要意义，但现有模型往往未能全面考虑这些因素的耦合作用。

Method: 研究提出一个结合弹性、粘性和多孔效应与生长/收缩的数学模型。方法包括：1）分析连续和半离散版本模型的平衡稳定性；2）描述数值解的单调性条件；3）开发稳定化方法以避免数值解中的伪振荡；4）通过计算机模拟验证稳定化效果；5）评估总变差作为稳定化参数的函数以提供定量分析。

Result: 研究获得了以下结果：1）分析了连续和半离散模型平衡状态的稳定性；2）推导出数值解单调性的条件；3）开发了有效的稳定化方法以避免伪振荡；4）计算机模拟证实了稳定化方法的有效性；5）通过评估总变差随稳定化参数的变化，提供了定量分析框架。

Conclusion: 该研究成功建立了一个综合弹性、粘性、多孔效应与生长/收缩的数学模型，并提供了有效的数值稳定化方法。虽然未给出解的存在性证明，但通过稳定性分析和数值模拟验证了方法的有效性，为研究组织生长、肿瘤发展和皮肤收缩等生物过程提供了有价值的数学工具。

Abstract: We propose a mathematical model that combines elastic, viscous and porous effects with growth or shrinkage due to microstructural changes. This phenomenon is important in tissue or tumor growth, as well as in dermal contraction. Although existence results of the solution to the problem are not given, the current study assesses stability of the equilibria for both the continuous and semi-discrete versions of the model. Furthermore, a numerical condition for monotonicity of the numerical solution is described, as well as a way to stabilize the numerical solution so that spurious oscillations are avoided. The derived stabilization result is confirmed by computer simulations. In order to have a more quantitative picture, the total variation has been evaluated as a function of the stabilization parameter.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization](https://arxiv.org/abs/2512.09972)
*Kesheng Chen,Wenjian Luo,Zhenqian Zhu,Yamin Hu,Yiya Xi*

Main category: cs.LG

TL;DR: BAMBO是一个自动构建LLM帕累托集的新框架，通过混合最优块划分策略解决现有方法在稀疏解和高维搜索空间之间的两难问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型融合技术在构建帕累托集方面存在不足：粗粒度的模型级方法只能产生稀疏的次优解集，而细粒度的层级方法面临"维度灾难"，导致搜索空间计算不可行。

Method: 提出BAMBO框架，采用混合最优块划分策略，将其表述为一维聚类问题，利用动态规划方法优化平衡块内同质性和块间信息分布，显著降低维度而不牺牲关键粒度。整个过程在由q-期望超体积改进获取函数驱动的进化循环中自动化进行。

Result: 实验表明，BAMBO能够发现比基线方法更优越、更全面的帕累托前沿，使模型选择能够灵活适应不同的操作约束。

Conclusion: BAMBO成功解决了现有融合技术在构建LLM帕累托集方面的局限性，通过智能的块划分策略实现了高效且全面的模型能力-效率权衡探索。

Abstract: Constructing a Pareto set is pivotal for navigating the capability-efficiency trade-offs in Large Language Models (LLMs); however, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the "curse of dimensionality," rendering the search space computationally intractable. To resolve this dichotomy, we propose BAMBO (Bayesian Adaptive Multi-objective Block-wise Optimization), a novel framework that automatically constructs the LLM Pareto set. BAMBO renders the search tractable by introducing a Hybrid Optimal Block Partitioning strategy. Formulated as a 1D clustering problem, this strategy leverages a dynamic programming approach to optimally balance intra-block homogeneity and inter-block information distribution, thereby dramatically reducing dimensionality without sacrificing critical granularity. The entire process is automated within an evolutionary loop driven by the q-Expected Hypervolume Improvement (qEHVI) acquisition function. Experiments demonstrate that BAMBO discovers a superior and more comprehensive Pareto frontier than baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/xin8coder/BAMBO.

</details>


### [24] [Latent Action World Models for Control with Unlabeled Trajectories](https://arxiv.org/abs/2512.10016)
*Marvin Alles,Xingyuan Zhang,Patrick van der Smagt,Philip Becker-Ehmck*

Main category: cs.LG

TL;DR: 提出了一种潜在动作世界模型，能够同时利用动作标注和无动作数据，通过共享潜在动作表示显著减少对标注数据的需求，在DeepMind Control Suite上仅需十分之一的标注样本即可达到强性能。


<details>
  <summary>Details</summary>
Motivation: 人类学习结合直接交互和无动作经验（如观看视频），但标准世界模型通常依赖动作标注轨迹，当动作标签稀缺时效果受限。需要一种能同时利用动作标注和无动作数据的方法。

Method: 引入潜在动作世界模型家族，学习共享潜在动作表示。该潜在空间将观测到的控制信号与从被动观测推断的动作对齐，使单个动态模型能够在大规模无标签轨迹上训练，同时仅需少量动作标注样本。使用潜在动作世界模型通过离线强化学习学习潜在动作策略。

Result: 在DeepMind Control Suite上，该方法仅需约十分之一的动作标注样本就能达到与纯动作标注基线相当的强性能。证明了潜在动作能够有效结合被动和交互数据进行训练。

Conclusion: 潜在动作世界模型能够桥接离线强化学习和无动作训练两个传统分离的领域，使世界模型学习更高效。通过共享潜在动作表示，模型能够充分利用大规模无标签数据，显著减少对昂贵动作标注的需求。

Abstract: Inspired by how humans combine direct interaction with action-free experience (e.g., videos), we study world models that learn from heterogeneous data. Standard world models typically rely on action-conditioned trajectories, which limits effectiveness when action labels are scarce. We introduce a family of latent-action world models that jointly use action-conditioned and action-free data by learning a shared latent action representation. This latent space aligns observed control signals with actions inferred from passive observations, enabling a single dynamics model to train on large-scale unlabeled trajectories while requiring only a small set of action-labeled ones. We use the latent-action world model to learn a latent-action policy through offline reinforcement learning (RL), thereby bridging two traditionally separate domains: offline RL, which typically relies on action-conditioned data, and action-free training, which is rarely used with subsequent RL. On the DeepMind Control Suite, our approach achieves strong performance while using about an order of magnitude fewer action-labeled samples than purely action-conditioned baselines. These results show that latent actions enable training on both passive and interactive data, which makes world models learn more efficiently.

</details>


### [25] [Cluster-Dags as Powerful Background Knowledge For Causal Discovery](https://arxiv.org/abs/2512.10032)
*Jan Marco Ruiz de Vargas,Kirtan Padh,Niki Kilbertus*

Main category: cs.LG

TL;DR: 本文提出利用Cluster-DAGs作为先验知识框架来预热因果发现，并引入两种改进的基于约束的算法Cluster-PC和Cluster-FCI，在模拟数据上验证了其优于无先验知识的基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前因果发现方法在处理高维数据和复杂依赖关系时面临挑战，而融入系统先验知识可以辅助因果发现。本文旨在利用Cluster-DAGs作为先验知识框架来改进因果发现过程。

Method: 提出Cluster-DAGs作为先验知识框架，比现有的基于分层背景知识的方法更具灵活性。在此基础上引入了两种改进的基于约束的算法：Cluster-PC用于完全观测设置，Cluster-FCI用于部分观测设置。

Result: 在模拟数据上的实证评估表明，Cluster-PC和Cluster-FCI算法在各自设置下均优于没有先验知识的相应基线方法。

Conclusion: Cluster-DAGs作为先验知识框架能够有效改进因果发现，特别是在高维和复杂依赖场景下，提出的Cluster-PC和Cluster-FCI算法展现了优越性能。

Abstract: Finding cause-effect relationships is of key importance in science. Causal discovery aims to recover a graph from data that succinctly describes these cause-effect relationships. However, current methods face several challenges, especially when dealing with high-dimensional data and complex dependencies. Incorporating prior knowledge about the system can aid causal discovery. In this work, we leverage Cluster-DAGs as a prior knowledge framework to warm-start causal discovery. We show that Cluster-DAGs offer greater flexibility than existing approaches based on tiered background knowledge and introduce two modified constraint-based algorithms, Cluster-PC and Cluster-FCI, for causal discovery in the fully and partially observed setting, respectively. Empirical evaluation on simulated data demonstrates that Cluster-PC and Cluster-FCI outperform their respective baselines without prior knowledge.

</details>


### [26] [Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation](https://arxiv.org/abs/2512.10033)
*Sarwan Ali*

Main category: cs.LG

TL;DR: HB-SGE是一种结合重球动量和预测梯度外推的鲁棒一阶优化方法，在病态和非凸问题上比NAG和标准动量方法更稳定，收敛性更好。


<details>
  <summary>Details</summary>
Motivation: NAG等加速梯度方法在条件良好的问题上收敛快，但在病态或非凸问题上由于动量积累过于激进而容易发散。需要一种既能保持加速效果又能在复杂地形上保持稳定的优化方法。

Method: HB-SGE结合了重球动量和预测梯度外推技术。不同于传统动量方法积累历史梯度，HB-SGE使用局部泰勒近似来估计未来梯度方向，提供自适应加速同时保持稳定性。

Result: 在病态二次问题（条件数κ=50）上，HB-SGE在119次迭代内收敛，而SGD和NAG都发散。在非凸Rosenbrock函数上，HB-SGE在2,718次迭代内收敛，而经典动量方法在10步内就发散。HB-SGE在条件良好问题上不如NAG快，但在各种地形上都比SGD有加速效果。

Conclusion: HB-SGE提供了一种鲁棒的优化替代方案，在保持与标准动量相同超参数和O(d)内存开销的同时，能在病态和非凸问题上稳定收敛，而传统加速方法在这些问题上容易发散。

Abstract: Accelerated gradient methods like Nesterov's Accelerated Gradient (NAG) achieve faster convergence on well-conditioned problems but often diverge on ill-conditioned or non-convex landscapes due to aggressive momentum accumulation. We propose Heavy-Ball Synthetic Gradient Extrapolation (HB-SGE), a robust first-order method that combines heavy-ball momentum with predictive gradient extrapolation. Unlike classical momentum methods that accumulate historical gradients, HB-SGE estimates future gradient directions using local Taylor approximations, providing adaptive acceleration while maintaining stability. We prove convergence guarantees for strongly convex functions and demonstrate empirically that HB-SGE prevents divergence on problems where NAG and standard momentum fail. On ill-conditioned quadratics (condition number $κ=50$), HB-SGE converges in 119 iterations while both SGD and NAG diverge. On the non-convex Rosenbrock function, HB-SGE achieves convergence in 2,718 iterations where classical momentum methods diverge within 10 steps. While NAG remains faster on well-conditioned problems, HB-SGE provides a robust alternative with speedup over SGD across diverse landscapes, requiring only $O(d)$ memory overhead and the same hyperparameters as standard momentum.

</details>


### [27] [Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs](https://arxiv.org/abs/2512.10040)
*Skyler Wu,Aymen Echarghaoui*

Main category: cs.LG

TL;DR: 论文提出四种新的权重分配策略来改进多参考偏好优化方法，但实验发现单参考DPO方法通常优于多参考方法。


<details>
  <summary>Details</summary>
Motivation: 当前多参考偏好优化方法中的参考权重设置是临时性的且缺乏统计依据，导致性能不可靠，需要更系统的方法来利用多个参考模型的集体优势。

Method: 提出了四种新的权重分配策略：两种离线方法利用验证集信号；一种在线方法使用滑动窗口估计器减少过拟合；另一种在线方法将参考权重分配视为K臂老虎机问题，采用Thompson Sampling方法。

Result: 实验使用Qwen2.5-0.5B作为策略模型，7个不同家族的参考模型，在UltraFeedback和SafeRLHF数据集上，所有4种新策略都优于当前的MRPO权重方法。但令人深思的是，单参考DPO方法（使用7个参考中的任意6个）持续优于所有测试的多参考方法。

Conclusion: 虽然提出的新权重分配策略改进了多参考偏好优化方法，但单参考DPO方法的持续优越性对多参考方法的实际吸引力提出了质疑，需要重新评估多参考方法的实用价值。

Abstract: Fine-tuning is integral for aligning large language models (LLMs) with human preferences. Multiple-Reference Preference Optimization (MRPO) builds on Direct Preference Optimization (DPO) by fine-tuning LLMs on preference datasets while regularizing the policy towards a mixture of reference models to leverage their collective desirable properties. However, current methods for setting the reference weights are ad-hoc and statistically unsound, leading to unreliable performance. To address this, we introduce four new weighting strategies: two offline methods that leverage held-out validation signal; one online method that uses a sliding-window estimator to reduce overfitting; and an online method that treats reference weighting as a $K$-armed bandit via Thompson Sampling. Experiments using Qwen2.5-0.5B as the policy model and seven reference models from the Llama, Mistral, Qwen, Yi, and Phi families (0.5B-14B each) show that all 4 of our strategies outperform the current MRPO weighting methods on UltraFeedback and SafeRLHF in preference accuracy. More thought-provokingly, however, we find that single-reference DPO, using any of 6 out of 7 references, consistently outperforms all tested multiple-reference approaches -- calling into question the practical appeal of multiple-reference approaches.

</details>


### [28] [SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation](https://arxiv.org/abs/2512.10042)
*Jongmin Lee,Meiqi Sun,Pieter Abbeel*

Main category: cs.LG

TL;DR: SEMDICE是一种无监督强化学习预训练方法，通过最大化状态熵来学习先验策略，可直接从任意离策略数据集中计算最优策略


<details>
  <summary>Details</summary>
Motivation: 在无监督强化学习预训练中，智能体需要在不依赖任务特定奖励函数的情况下学习适用于下游任务的先验策略。状态熵最大化（SEM）方法旨在学习最大化状态平稳分布熵的策略，但现有方法存在局限性。

Method: 提出SEMDICE算法，这是一种原则性的离策略算法，可直接从任意离策略数据集中计算SEM策略。该算法在平稳分布空间中直接优化策略，计算单一、平稳的马尔可夫状态熵最大化策略。

Result: 实验结果表明，SEMDICE在最大化状态熵方面优于基线算法，同时在基于SEM的无监督RL预训练方法中实现了最佳的下游任务适应效率。

Conclusion: SEMDICE提供了一种有效的无监督预训练方法，能够从任意离策略数据集中学习高质量的先验策略，为下游任务提供更好的适应基础。

Abstract: In the unsupervised pre-training for reinforcement learning, the agent aims to learn a prior policy for downstream tasks without relying on task-specific reward functions. We focus on state entropy maximization (SEM), where the goal is to learn a policy that maximizes the entropy of the state stationary distribution. In this paper, we introduce SEMDICE, a principled off-policy algorithm that computes an SEM policy from an arbitrary off-policy dataset, which optimizes the policy directly within the space of stationary distributions. SEMDICE computes a single, stationary Markov state-entropy-maximizing policy from an arbitrary off-policy dataset. Experimental results demonstrate that SEMDICE outperforms baseline algorithms in maximizing state entropy while achieving the best adaptation efficiency for downstream tasks among SEM-based unsupervised RL pre-training methods.

</details>


### [29] [Local LLM Ensembles for Zero-shot Portuguese Named Entity Recognition](https://arxiv.org/abs/2512.10043)
*João Lucas Luz Lima Sarcinelli,Diego Furtado Silva*

Main category: cs.LG

TL;DR: 提出一种新颖的三步集成方法，使用本地运行的中等规模LLM进行零样本命名实体识别，在葡萄牙语NER任务中优于单个模型


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理任务中表现出色，但在命名实体识别（尤其是低资源语言如葡萄牙语）方面表现不佳。虽然开源模型支持本地部署，但没有单一模型在所有任务中都占优，这促使了集成方法的研究。然而，现有的LLM集成主要关注文本生成或分类，NER任务研究不足。

Method: 提出三步集成流程：1）使用启发式方法选择最优模型组合，2）利用少量标注数据，3）在零样本设置下结合多个本地运行的中等规模LLM进行NER。该方法不需要对模型进行微调。

Result: 在五个葡萄牙语NER数据集中，该方法在四个数据集上优于单个LLM。此外，在不同源数据集上获得的集成模型在跨数据集配置中通常优于单个LLM，可能消除了对当前任务标注数据的需求。

Conclusion: 该工作通过有效结合多个小型LLM而不需要微调，推进了可扩展、低资源和零样本NER的发展，为资源有限的语言提供了实用的NER解决方案。

Abstract: Large Language Models (LLMs) excel in many Natural Language Processing (NLP) tasks through in-context learning but often under-perform in Named Entity Recognition (NER), especially for lower-resource languages like Portuguese. While open-weight LLMs enable local deployment, no single model dominates all tasks, motivating ensemble approaches. However, existing LLM ensembles focus on text generation or classification, leaving NER under-explored. In this context, this work proposes a novel three-step ensemble pipeline for zero-shot NER using similarly capable, locally run LLMs. Our method outperforms individual LLMs in four out of five Portuguese NER datasets by leveraging a heuristic to select optimal model combinations with minimal annotated data. Moreover, we show that ensembles obtained on different source datasets generally outperform individual LLMs in cross-dataset configurations, potentially eliminating the need for annotated data for the current task. Our work advances scalable, low-resource, and zero-shot NER by effectively combining multiple small LLMs without fine-tuning. Code is available at https://github.com/Joao-Luz/local-llm-ner-ensemble.

</details>


### [30] [Detailed balance in large language model-driven agents](https://arxiv.org/abs/2512.10047)
*Zhuo-Yang Song,Qing-Hong Cao,Ming-xing Luo,Hua Xing Zhu*

Main category: cs.LG

TL;DR: 该论文发现大语言模型（LLM）驱动的智能体在状态转换中存在详细平衡，表明LLM生成可能不是通过学习规则集和策略，而是通过隐式学习一类超越不同LLM架构和提示模板的潜在函数。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM驱动的智能体在解决复杂问题方面取得了经验性成功，但缺乏理解其宏观动态的理论框架。本研究旨在建立复杂AI系统的宏观动力学理论，将AI智能体研究从工程实践提升为可预测、可量化的科学。

Method: 基于最小作用原理，通过实验测量LLM生成状态之间的转换概率，统计发现LLM生成转换中存在详细平衡。

Result: 实验发现LLM生成转换中存在详细平衡，表明LLM生成可能通过隐式学习一类底层潜在函数实现，这类函数可能超越不同的LLM架构和提示模板。这是首次发现不依赖于具体模型细节的LLM生成动力学宏观物理定律。

Conclusion: 该研究为建立复杂AI系统的宏观动力学理论做出了尝试，旨在将AI智能体研究从工程实践集合提升为基于有效测量的科学，这些测量是可预测和可量化的。

Abstract: Large language model (LLM)-driven agents are emerging as a powerful new paradigm for solving complex problems. Despite the empirical success of these practices, a theoretical framework to understand and unify their macroscopic dynamics remains lacking. This Letter proposes a method based on the least action principle to estimate the underlying generative directionality of LLMs embedded within agents. By experimentally measuring the transition probabilities between LLM-generated states, we statistically discover a detailed balance in LLM-generated transitions, indicating that LLM generation may not be achieved by generally learning rule sets and strategies, but rather by implicitly learning a class of underlying potential functions that may transcend different LLM architectures and prompt templates. To our knowledge, this is the first discovery of a macroscopic physical law in LLM generative dynamics that does not depend on specific model details. This work is an attempt to establish a macroscopic dynamics theory of complex AI systems, aiming to elevate the study of AI agents from a collection of engineering practices to a science built on effective measurements that are predictable and quantifiable.

</details>


### [31] [DB2-TransF: All You Need Is Learnable Daubechies Wavelets for Time Series Forecasting](https://arxiv.org/abs/2512.10051)
*Moulik Gupta,Achyut Mani Tripathi*

Main category: cs.LG

TL;DR: DB2-TransF是一种新型Transformer架构，用可学习的Daubechies小波系数层替代自注意力机制，用于时间序列预测，在保持预测精度的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在时间序列预测中虽然能有效建模长程依赖，但其二次计算复杂度限制了在大规模高维场景下的可扩展性和适应性，需要更高效的架构。

Method: 提出DB2-TransF架构，用可学习的Daubechies小波系数层替代自注意力机制，该小波模块能有效捕捉多尺度局部和全局模式，并增强多时间序列间的相关性建模。

Result: 在13个标准预测基准测试中，DB2-TransF达到与传统Transformer相当或更优的预测精度，同时显著降低了内存使用量，证明了其可扩展性和资源效率。

Conclusion: DB2-TransF为高级时间序列预测提供了一个可扩展且资源高效的框架，通过小波变换替代自注意力机制，在保持性能的同时解决了Transformer的计算瓶颈问题。

Abstract: Time series forecasting requires models that can efficiently capture complex temporal dependencies, especially in large-scale and high-dimensional settings. While Transformer-based architectures excel at modeling long-range dependencies, their quadratic computational complexity poses limitations on scalability and adaptability. To overcome these challenges, we introduce DB2-TransF, a novel Transformer-inspired architecture that replaces the self-attention mechanism with a learnable Daubechies wavelet coefficient layer. This wavelet-based module efficiently captures multi-scale local and global patterns and enhances the modeling of correlations across multiple time series for the time series forecasting task. Extensive experiments on 13 standard forecasting benchmarks demonstrate that DB2-TransF achieves comparable or superior predictive accuracy to conventional Transformers, while substantially reducing memory usage for the time series forecasting task. The obtained experimental results position DB2-TransF as a scalable and resource-efficient framework for advanced time series forecasting. Our code is available at https://github.com/SteadySurfdom/DB2-TransF

</details>


### [32] [Mitigating Exposure Bias in Risk-Aware Time Series Forecasting with Soft Tokens](https://arxiv.org/abs/2512.10056)
*Alireza Namazi,Amirreza Dolatpour Fathkouhi,Heman Shakeri*

Main category: cs.LG

TL;DR: SoTra方法通过传播连续概率分布来缓解自回归预测中的暴露偏差，学习校准的、不确定性感知的轨迹，并结合风险感知解码模块最小化临床风险，在血糖和血压预测中显著降低临床风险。


<details>
  <summary>Details</summary>
Motivation: 在糖尿病和血流动力学管理的预测控制中，不同操作区域具有不同的临床风险。标准的教师强制训练模型存在暴露偏差问题，导致闭环使用中的多步预测不稳定。

Method: 提出Soft-Token Trajectory Forecasting (SoTra)方法，传播连续概率分布（"软标记"）来缓解暴露偏差，学习校准的、不确定性感知的轨迹。然后通过风险感知解码模块最小化预期临床危害。

Result: 在血糖预测中，SoTra将基于区域的平均风险降低了18%；在血压预测中，将有效临床风险降低了约15%。

Conclusion: SoTra方法在安全关键的预测控制中表现出优越性能，支持其在临床环境中的应用。

Abstract: Autoregressive forecasting is central to predictive control in diabetes and hemodynamic management, where different operating zones carry different clinical risks. Standard models trained with teacher forcing suffer from exposure bias, yielding unstable multi-step forecasts for closed-loop use. We introduce Soft-Token Trajectory Forecasting (SoTra), which propagates continuous probability distributions (``soft tokens'') to mitigate exposure bias and learn calibrated, uncertainty-aware trajectories. A risk-aware decoding module then minimizes expected clinical harm. In glucose forecasting, SoTra reduces average zone-based risk by 18\%; in blood-pressure forecasting, it lowers effective clinical risk by approximately 15\%. These improvements support its use in safety-critical predictive control.

</details>


### [33] [MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis](https://arxiv.org/abs/2512.10098)
*Midhat Urooj,Ayan Banerjee,Farhat Shaikh,Kuntal Thakur,Sandeep Gupta*

Main category: cs.LG

TL;DR: MedXAI是一个基于专家知识的可解释医学影像分类框架，通过整合深度学习模型与临床专家知识，提升跨域泛化能力，减少罕见类别偏见，并提供临床可理解的解释。


<details>
  <summary>Details</summary>
Motivation: 医学AI在图像诊断中面临三大挑战：1）在域偏移下泛化能力差；2）对罕见病理类别存在偏见；3）缺乏临床部署所需的透明度。现有深度学习方法难以应对真实世界分布变化，且依赖技术性后处理解释方法（如显著性图、LIME）缺乏临床可理解性。

Method: 提出MedXAI框架，将深度视觉模型与临床专家知识相结合。框架通过定位相关诊断特征而非依赖技术性后处理方法，提供人类可理解的解释。符号组件作为有效的临床先验和正则化器，提升分布偏移下的鲁棒性。

Result: 在10个多中心数据集上评估，涵盖两个挑战性任务：1）静息态fMRI的癫痫发作区定位；2）糖尿病视网膜病变分级。结果显示：跨域泛化能力提升3%，罕见类别F1分数提升10%，显著优于强深度学习基线。消融实验证实符号组件作为临床先验和正则化器的有效性。

Conclusion: MedXAI在实现优越的域内和跨域性能的同时，提供临床对齐的解释，特别适用于多模态医学AI中的罕见疾病诊断，为安全关键临床环境中的部署提供了更可靠和可解释的解决方案。

Abstract: Accurate and interpretable image-based diagnosis remains a fundamental challenge in medical AI, particularly under domain shifts and rare-class conditions. Deep learning models often struggle with real-world distribution changes, exhibit bias against infrequent pathologies, and lack the transparency required for deployment in safety-critical clinical environments. We introduce MedXAI (An Explainable Framework for Medical Imaging Classification), a unified expert knowledge based framework that integrates deep vision models with clinician-derived expert knowledge to improve generalization, reduce rare-class bias, and provide human-understandable explanations by localizing the relevant diagnostic features rather than relying on technical post-hoc methods (e.g., Saliency Maps, LIME). We evaluate MedXAI across heterogeneous modalities on two challenging tasks: (i) Seizure Onset Zone localization from resting-state fMRI, and (ii) Diabetic Retinopathy grading. Ex periments on ten multicenter datasets show consistent gains, including a 3% improvement in cross-domain generalization and a 10% improvmnet in F1 score of rare class, substantially outperforming strong deep learning baselines. Ablations confirm that the symbolic components act as effective clinical priors and regularizers, improving robustness under distribution shift. MedXAI delivers clinically aligned explanations while achieving superior in-domain and cross-domain performance, particularly for rare diseases in multimodal medical AI.

</details>


### [34] [CHyLL: Learning Continuous Neural Representations of Hybrid Systems](https://arxiv.org/abs/2512.10117)
*Sangli Teng,Hang Liu,Jingyu Song,Koushil Sreenath*

Main category: cs.LG

TL;DR: CHyLL提出了一种在潜在空间中学习混合系统连续神经表示的方法，无需轨迹分割、事件函数或模式切换，通过将状态空间重构为分段光滑商流形使流变得空间连续。


<details>
  <summary>Details</summary>
Motivation: 现有方法学习每个离散模式中的动态，但受到模式切换和流中不连续性的组合影响。学习同时具有连续和离散时间动态的混合系统的流具有挑战性。

Method: CHyLL的关键洞察是重置映射在守卫表面粘合状态空间，将状态空间重新表述为分段光滑商流形，使流变得空间连续。基于微分拓扑的嵌入定理，CHyLL同时学习高维空间中的无奇点神经嵌入及其中的连续流。

Result: CHyLL能够准确预测混合系统的流，具有卓越的准确性，并能识别混合系统的拓扑不变量。最后将CHyLL应用于随机最优控制问题。

Conclusion: CHyLL提供了一种无需轨迹分割、事件函数或模式切换的连续混合系统学习方法，通过将状态空间重构为商流形并在潜在空间中学习连续流，有效解决了混合系统学习中的不连续性问题。

Abstract: Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.

</details>


### [35] [Partitioning the Sample Space for a More Precise Shannon Entropy Estimation](https://arxiv.org/abs/2512.10133)
*Gabriel F. A. Bastos,Jugurta Montalvão*

Main category: cs.LG

TL;DR: 提出一种新的离散熵估计器，通过结合缺失质量和未见结果数量的估计来补偿负偏差，在小数据集情况下优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 在小数据集情况下（样本数可能小于可能结果数），可靠地估计香农熵是一个关键问题，但现有方法存在负偏差问题。

Method: 利用可分解性特性，结合缺失质量和未见结果数量的估计来补偿负偏差，提出新的离散熵估计器。

Result: 在欠采样情况下，该方法优于一些经典估计器，性能与一些成熟的最先进估计器相当。

Conclusion: 提出的离散熵估计器在小数据集情况下能有效补偿负偏差，为香农熵的可靠估计提供了有效解决方案。

Abstract: Reliable data-driven estimation of Shannon entropy from small data sets, where the number of examples is potentially smaller than the number of possible outcomes, is a critical matter in several applications. In this paper, we introduce a discrete entropy estimator, where we use the decomposability property in combination with estimations of the missing mass and the number of unseen outcomes to compensate for the negative bias induced by them. Experimental results show that the proposed method outperforms some classical estimators in undersampled regimes, and performs comparably with some well-established state-of-the-art estimators.

</details>


### [36] [Sequence-to-Image Transformation for Sequence Classification Using Rips Complex Construction and Chaos Game Representation](https://arxiv.org/abs/2512.10141)
*Sarwan Ali,Taslim Murad,Imdadullah Khan*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的拓扑方法，将分子序列通过混沌游戏表示和Rips复形构造转化为图像，解决了传统特征工程稀疏性和深度学习在表格生物数据上表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 传统分子序列分类的特征工程方法存在稀疏性问题和计算复杂度高的问题，而深度学习模型在表格生物数据上往往表现不佳。需要一种既能保留序列关键信息，又能有效利用基于视觉的深度学习架构的新方法。

Method: 结合混沌游戏表示和代数拓扑中的Rips复形构造，将分子序列元素映射到2D坐标，计算成对距离，构建Rips复形来捕捉局部结构和全局拓扑特征。

Result: 在抗癌肽数据集上的实验表明，该方法优于基于向量的方法、序列语言模型和现有的基于图像的方法，在乳腺癌和肺癌数据集上分别达到86.8%和94.5%的准确率。

Conclusion: 拓扑表示保留了关键的序列信息，同时能够有效利用基于视觉的深度学习架构进行分子序列分析，为分子序列分类提供了新的有效方法。

Abstract: Traditional feature engineering approaches for molecular sequence classification suffer from sparsity issues and computational complexity, while deep learning models often underperform on tabular biological data. This paper introduces a novel topological approach that transforms molecular sequences into images by combining Chaos Game Representation (CGR) with Rips complex construction from algebraic topology. Our method maps sequence elements to 2D coordinates via CGR, computes pairwise distances, and constructs Rips complexes to capture both local structural and global topological features. We provide formal guarantees on representation uniqueness, topological stability, and information preservation. Extensive experiments on anticancer peptide datasets demonstrate superior performance over vector-based, sequence language models, and existing image-based methods, achieving 86.8\% and 94.5\% accuracy on breast and lung cancer datasets, respectively. The topological representation preserves critical sequence information while enabling effective utilization of vision-based deep learning architectures for molecular sequence analysis.

</details>


### [37] [Murmur2Vec: A Hashing Based Solution For Embedding Generation Of COVID-19 Spike Sequences](https://arxiv.org/abs/2512.10147)
*Sarwan Ali,Taslim Murad*

Main category: cs.LG

TL;DR: 提出基于哈希的SARS-CoV-2刺突蛋白序列嵌入方法，用于高效的大规模病毒谱系分类，相比现有方法显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: COVID-19早期检测和特征分析对临床响应和公共卫生规划至关重要。现有方法存在计算成本高、扩展性差的问题：系统发育树方法计算密集，无法扩展到数百万序列数据集；现有嵌入方法依赖序列对齐或预测性能不佳、运行成本高

Method: 针对SARS-CoV-2刺突蛋白区域的最常见谱系，提出可扩展的嵌入方法，利用哈希技术生成紧凑的低维序列表示，然后用这些嵌入训练多种机器学习模型进行监督谱系分类

Result: 与多种基准和最先进的生物序列嵌入方法进行广泛评估比较，结果显示所提嵌入方法在效率上有显著改进，分类准确率达到86.4%，同时将嵌入生成时间减少高达99.81%

Conclusion: 该方法为大规模病毒序列分析提供了一个快速、有效且可扩展的解决方案，具有实际应用潜力

Abstract: Early detection and characterization of coronavirus disease (COVID-19), caused by SARS-CoV-2, remain critical for effective clinical response and public-health planning. The global availability of large-scale viral sequence data presents significant opportunities for computational analysis; however, existing approaches face notable limitations. Phylogenetic tree-based methods are computationally intensive and do not scale efficiently to today's multi-million-sequence datasets. Similarly, current embedding-based techniques often rely on aligned sequences or exhibit suboptimal predictive performance and high runtime costs, creating barriers to practical large-scale analysis. In this study, we focus on the most prevalent SARS-CoV-2 lineages associated with the spike protein region and introduce a scalable embedding method that leverages hashing to generate compact, low-dimensional representations of spike sequences. These embeddings are subsequently used to train a variety of machine learning models for supervised lineage classification. We conduct an extensive evaluation comparing our approach with multiple baseline and state-of-the-art biological sequence embedding methods across diverse metrics. Our results demonstrate that the proposed embeddings offer substantial improvements in efficiency, achieving up to 86.4\% classification accuracy while reducing embedding generation time by as much as 99.81\%. This highlights the method's potential as a fast, effective, and scalable solution for large-scale viral sequence analysis.

</details>


### [38] [CIEGAD: Cluster-Conditioned Interpolative and Extrapolative Framework for Geometry-Aware and Domain-Aligned Data Augmentation](https://arxiv.org/abs/2512.10178)
*Keito Inoshita,Xiaokang Zhou,Akira Kawai,Katsutoshi Yada*

Main category: cs.LG

TL;DR: CIEGAD是一个面向实际部署的深度学习数据增强框架，通过聚类条件、插值和外推合成，系统性地补充真实数据分布中语义未覆盖的区域，特别针对长尾和多类分类任务。


<details>
  <summary>Details</summary>
Motivation: 实际深度学习部署中，数据稀缺和标签分布不平衡导致真实数据分布中存在语义未覆盖区域，这阻碍模型训练，导致类边界附近误分类以及边缘区域行为不稳定。尽管大语言模型在数据增强方面有潜力，但尚未建立能够同时实现生成方向控制、领域对齐和质量控制的集成框架。

Method: CIEGAD框架包含：1）通过聚类条件构建领域配置文件；2）使用集成类别频率和几何指标的层次频率-几何分配进行生成分配；3）通过插值和外推合成的共存精细控制生成方向；4）通过几何约束过滤结合LLM-as-a-Judge机制进行质量控制。

Result: 在多个分类任务上的实验表明，CIEGAD能有效扩展真实数据分布的边缘，同时保持生成数据与真实数据的高对齐度以及语义多样性。特别在长尾和多类分类任务中，CIEGAD持续提升F1分数和召回率，验证了分布一致性、多样性和质量的三重和谐。

Conclusion: CIEGAD作为一个面向实际的数据增强框架，能够补充代表性不足的区域，同时保持与真实数据的对齐，为解决实际部署中的数据稀缺和分布不平衡问题提供了有效方案。

Abstract: In practical deep learning deployment, the scarcity of data and the imbalance of label distributions often lead to semantically uncovered regions within the real-world data distribution, hindering model training and causing misclassification near class boundaries as well as unstable behaviors in peripheral areas. Although recent large language models (LLMs) show promise for data augmentation, an integrated framework that simultaneously achieves directional control of generation, domain alignment, and quality control has not yet been fully established. To address these challenges, we propose a Cluster-conditioned Interpolative and Extrapolative framework for Geometry-Aware and Domain-aligned data augmentation (CIEGAD), which systematically complements both in-distribution and out-of-distribution semantically uncovered regions. CIEGAD constructs domain profiles through cluster conditioning, allocates generation with a hierarchical frequency-geometric allocation integrating class frequency and geometric indicators, and finely controls generation directions via the coexistence of interpolative and extrapolative synthesis. It further performs quality control through geometry-constrained filtering combined with an LLM-as-a-Judge mechanism. Experiments on multiple classification tasks demonstrate that CIEGAD effectively extends the periphery of real-world data distributions while maintaining high alignment between generated and real-world data as well as semantic diversity. In particular, for long-tailed and multi-class classification tasks, CIEGAD consistently improves F1 and recall, validating the triple harmony of distributional consistency, diversity, and quality. These results indicate that CIEGAD serves as a practically oriented data augmentation framework that complements underrepresented regions while preserving alignment with real-world data.

</details>


### [39] [MiniF2F-Dafny: LLM-Guided Mathematical Theorem Proving via Auto-Active Verification](https://arxiv.org/abs/2512.10187)
*Mantas Baksys,Stefan Zetzsche,Olivier Bouissou*

Main category: cs.LG

TL;DR: 将数学推理基准miniF2F首次翻译到自动化定理证明器Dafny，评估其自动验证能力和LLM辅助证明效果


<details>
  <summary>Details</summary>
Motivation: 将数学推理基准从交互式定理证明器扩展到自动化定理证明器，探索自动化验证与LLM辅助证明的结合

Method: 创建miniF2F-Dafny基准，测试Dafny自动验证能力，评估12个现成LLM在提供证明提示方面的表现，采用迭代错误校正方法

Result: Dafny自动验证了测试集40.6%和验证集44.7%的问题；最佳LLM在pass@4指标下达到55.7%成功率；展示了LLM高层指导与自动化底层处理的协同效应

Conclusion: 首次将数学推理基准扩展到自动化定理证明器，验证了LLM与自动化定理证明器结合的有效性，为数学推理自动化提供了新方向

Abstract: We present miniF2F-Dafny, the first translation of the mathematical reasoning benchmark miniF2F to an automated theorem prover: Dafny. Previously, the benchmark existed only in interactive theorem provers (Lean, Isabelle, HOL Light, Metamath). We find that Dafny's automation verifies 99/244 (40.6%) of the test set and 109/244 (44.7%) of the validation set with empty proofs--requiring no manual proof steps. For problems where empty proofs fail, we evaluate 12 off-the-shelf LLMs on providing proof hints. The best model we test achieves 55.7% pass@4 success rate employing iterative error correction. These preliminary results highlight an effective division of labor: LLMs provide high-level guidance while automation handles low-level details. Our benchmark can be found on GitHub at http://github.com/dafny-lang/miniF2F .

</details>


### [40] [Federated Domain Generalization with Latent Space Inversion](https://arxiv.org/abs/2512.10224)
*Ragja Palakkadavath,Hung Le,Thanh Nguyen-Tang,Svetha Venkatesh,Sunil Gupta*

Main category: cs.LG

TL;DR: 提出一种新的联邦域泛化方法，通过潜在空间反转技术保护客户端隐私，并使用重要权重聚合策略处理非独立同分布数据，在减少通信开销的同时提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦域泛化方法虽然提升了全局模型的泛化能力，但通过共享客户端数据统计信息的方式会损害隐私保护。同时，当客户端数据非独立同分布时，聚合本地模型可能会丢失重要的本地适应信息。

Method: 1. 提出潜在空间反转技术，通过强制本地模型间的域不变性来改善客户端隐私保护；2. 设计重要权重聚合策略，在模型聚合时优先考虑对本地模型预测有显著影响的参数。

Result: 实验表明，该方法在减少通信开销的同时，取得了优于现有最先进方法的性能表现。

Conclusion: 提出的联邦域泛化方法通过潜在空间反转和重要权重聚合，在保护隐私的同时有效处理非独立同分布数据，实现了更好的泛化性能和更低的通信成本。

Abstract: Federated domain generalization (FedDG) addresses distribution shifts among clients in a federated learning framework. FedDG methods aggregate the parameters of locally trained client models to form a global model that generalizes to unseen clients while preserving data privacy. While improving the generalization capability of the global model, many existing approaches in FedDG jeopardize privacy by sharing statistics of client data between themselves. Our solution addresses this problem by contributing new ways to perform local client training and model aggregation. To improve local client training, we enforce (domain) invariance across local models with the help of a novel technique, \textbf{latent space inversion}, which enables better client privacy. When clients are not \emph{i.i.d}, aggregating their local models may discard certain local adaptations. To overcome this, we propose an \textbf{important weight} aggregation strategy to prioritize parameters that significantly influence predictions of local models during aggregation. Our extensive experiments show that our approach achieves superior results over state-of-the-art methods with less communication overhead.

</details>


### [41] [Adaptive Information Routing for Multimodal Time Series Forecasting](https://arxiv.org/abs/2512.10229)
*Jun Seo,Hyeokjun Choe,Seohui Bae,Soyeon Park,Wonbin Ahn,Taeyoon Lim,Junhyuk Kang,Sangjun Han,Jaehoon Lee,Dongwan Kang,Minjae Kim,Sungdong Yoo,Soonyoung Lee*

Main category: cs.LG

TL;DR: 提出自适应信息路由（AIR）框架，利用文本信息动态引导时间序列模型，通过文本精炼管道处理原始文本数据，在原油价格和汇率等实际市场数据上显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测主要依赖历史数据，但在实际场景中，仅凭时间序列数据往往信息不足，难以实现准确预测。需要引入多模态方法，特别是结合文本数据来增强预测能力。

Method: 提出自适应信息路由（AIR）框架，不同于现有方法将文本数据与时间序列数据同等对待，AIR利用文本信息动态引导时间序列模型，控制多元时间序列信息的组合方式和程度。同时开发文本精炼管道，使用大语言模型将原始文本数据转换为适合多模态预测的形式，并建立相应的基准测试。

Result: 在原油价格和汇率等实际市场数据上的实验结果表明，AIR能够有效利用文本输入调节时间序列模型的行为，在各种时间序列预测任务中显著提高预测准确性。

Conclusion: AIR框架通过文本信息动态引导时间序列模型，结合文本精炼管道，为多模态时间序列预测提供了有效解决方案，在实际应用中展现出优越性能。

Abstract: Time series forecasting is a critical task for artificial intelligence with numerous real-world applications. Traditional approaches primarily rely on historical time series data to predict the future values. However, in practical scenarios, this is often insufficient for accurate predictions due to the limited information available. To address this challenge, multimodal time series forecasting methods which incorporate additional data modalities, mainly text data, alongside time series data have been explored. In this work, we introduce the Adaptive Information Routing (AIR) framework, a novel approach for multimodal time series forecasting. Unlike existing methods that treat text data on par with time series data as interchangeable auxiliary features for forecasting, AIR leverages text information to dynamically guide the time series model by controlling how and to what extent multivariate time series information should be combined. We also present a text-refinement pipeline that employs a large language model to convert raw text data into a form suitable for multimodal forecasting, and we introduce a benchmark that facilitates multimodal forecasting experiments based on this pipeline. Experiment results with the real world market data such as crude oil price and exchange rates demonstrate that AIR effectively modulates the behavior of the time series model using textual inputs, significantly enhancing forecasting accuracy in various time series forecasting tasks.

</details>


### [42] [R^2-HGP: A Double-Regularized Gaussian Process for Heterogeneous Transfer Learning](https://arxiv.org/abs/2512.10258)
*Duo Wang,Xinming Wang,Chao Wang,Xiaowei Yue,Jianguo Wu*

Main category: cs.LG

TL;DR: 提出R²-HGP框架，通过可训练先验概率映射对齐异构输入域，结合物理知识正则化和稀疏惩罚，解决多输出高斯过程在异构迁移学习中的三大挑战。


<details>
  <summary>Details</summary>
Motivation: 多输出高斯过程在迁移学习中面临三大挑战：1)源域和目标域输入空间异构导致知识直接迁移困难；2)忽略先验知识和物理信息，导致不稳定映射；3)源与目标间不当信息共享易引发负迁移。传统模型无法统一解决这些问题。

Method: 提出双正则化异构高斯过程框架(R²-HGP)：1)可训练先验概率映射模型对齐异构输入域；2)将对齐结果作为隐变量构建多源迁移GP模型；3)整合到条件变分自编码器框架；4)加入物理知识作为正则项确保对齐符合物理规律；5)对迁移系数施加稀疏惩罚，自适应选择信息量最大的源输出。

Result: 大量仿真和实际工程案例验证了R²-HGP的有效性，在多种评估指标上均优于现有最先进基准方法，表现出持续优越性。

Conclusion: R²-HGP框架成功解决了异构迁移学习中的三大挑战，通过输入域对齐、物理知识整合和自适应源选择，实现了更稳定、准确的知识迁移，避免了负迁移问题。

Abstract: Multi-output Gaussian process (MGP) models have attracted significant attention for their flexibility and uncertainty-quantification capabilities, and have been widely adopted in multi-source transfer learning scenarios due to their ability to capture inter-task correlations. However, they still face several challenges in transfer learning. First, the input spaces of the source and target domains are often heterogeneous, which makes direct knowledge transfer difficult. Second, potential prior knowledge and physical information are typically ignored during heterogeneous transfer, hampering the utilization of domain-specific insights and leading to unstable mappings. Third, inappropriate information sharing among target and sources can easily lead to negative transfer. Traditional models fail to address these issues in a unified way. To overcome these limitations, this paper proposes a Double-Regularized Heterogeneous Gaussian Process framework (R^2-HGP). Specifically, a trainable prior probability mapping model is first proposed to align the heterogeneous input domains. The resulting aligned inputs are treated as latent variables, upon which a multi-source transfer GP model is constructed and the entire structure is integrated into a novel conditional variational autoencoder (CVAE) based framework. Physical insights is further incorporated as a regularization term to ensure that the alignment results adhere to known physical knowledge. Next, within the multi-source transfer GP model, a sparsity penalty is imposed on the transfer coefficients, enabling the model to adaptively select the most informative source outputs and suppress negative transfer. Extensive simulations and real-world engineering case studies validate the effectiveness of our R^2-HGP, demonstrating consistent superiority over state-of-the-art benchmarks across diverse evaluation metrics.

</details>


### [43] [An Interpretable AI Tool for SAVR vs TAVR in Low to Intermediate Risk Patients with Severe Aortic Stenosis](https://arxiv.org/abs/2512.10308)
*Vasiliki Stoumpou,Maciej Tysarowski,Talhat Azemi,Jawad Haider,Howard L. Haronian,Robert C. Hagberg,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 开发可解释的处方框架，通过最优策略树为主动脉瓣狭窄患者提供个体化的SAVR或TAVR治疗推荐，以最小化5年死亡率。


<details>
  <summary>Details</summary>
Motivation: 目前临床实践中，对于低至中危严重主动脉瓣狭窄患者，手术（SAVR）与经导管（TAVR）主动脉瓣置换术的选择存在差异，主要受患者异质性和机构偏好影响。现有模型仅能预测术后风险，缺乏能够直接优化长期结果的可解释、个体化治疗推荐。

Method: 引入可解释的处方框架，整合预后匹配、反事实结果建模和最优策略树（OPT）。使用Hartford医院和St. Vincent's医院的数据，通过预后匹配和样本加权模拟随机化，估计SAVR和TAVR两种治疗下的反事实死亡率。策略模型基于这些反事实预测训练，将患者划分为临床一致的亚组，并推荐估计风险较低的治疗。

Result: 如果应用OPT推荐，反事实评估显示：Hartford医院的5年死亡率估计降低20.3%，St. Vincent's医院降低13.8%（相对于实际临床选择）。学习到的决策边界与现实世界结果和临床观察一致，显示出对不同机构未见数据的良好泛化能力。

Conclusion: 该可解释处方框架首次为TAVR与SAVR选择提供透明、数据驱动的推荐，在内部和外部队列中均改善了估计的长期结果，同时保持临床合理性，为结构性心脏病精准医学提供了更系统、循证的方法。

Abstract: Background. Treatment selection for low to intermediate risk patients with severe aortic stenosis between surgical (SAVR) and transcatheter (TAVR) aortic valve replacement remains variable in clinical practice, driven by patient heterogeneity and institutional preferences. While existing models predict postprocedural risk, there is a lack of interpretable, individualized treatment recommendations that directly optimize long-term outcomes.
  Methods. We introduce an interpretable prescriptive framework that integrates prognostic matching, counterfactual outcome modeling, and an Optimal Policy Tree (OPT) to recommend the treatment minimizing expected 5-year mortality. Using data from Hartford Hospital and St. Vincent's Hospital, we emulate randomization via prognostic matching and sample weighting and estimate counterfactual mortality under both SAVR and TAVR. The policy model, trained on these counterfactual predictions, partitions patients into clinically coherent subgroups and prescribes the treatment associated with lower estimated risk.
  Findings. If the OPT prescriptions are applied, counterfactual evaluation showed an estimated reduction in 5-year mortality of 20.3\% in Hartford and 13.8\% in St. Vincent's relative to real-life prescriptions, showing promising generalizability to unseen data from a different institution. The learned decision boundaries aligned with real-world outcomes and clinical observations.
  Interpretation. Our interpretable prescriptive framework is, to the best of our knowledge, the first to provide transparent, data-driven recommendations for TAVR versus SAVR that improve estimated long-term outcomes both in an internal and external cohort, while remaining clinically grounded and contributing toward a more systematic and evidence-based approach to precision medicine in structural heart disease.

</details>


### [44] [A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale](https://arxiv.org/abs/2512.10341)
*Vinoth Punniyamoorthy,Ashok Gadi Parthi,Mayilsamy Palanigounder,Ravi Kiran Kodali,Bikesh Kumar,Kabilan Kannan*

Main category: cs.LG

TL;DR: 提出一个云原生隐私保护架构，整合联邦学习、差分隐私、零知识合规证明和基于强化学习的自适应治理，用于分布式机器学习系统


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习系统需要强大的隐私保证、可验证的合规性以及在异构多云环境中的可扩展部署

Method: 集成联邦学习、差分隐私、零知识合规证明和基于强化学习的自适应治理的云原生隐私保护架构，支持在不集中敏感数据的情况下进行安全模型训练和推理

Result: 在混合Kubernetes集群上部署的原型显示降低了成员推断风险，保持了一致的正式隐私预算执行，并在差分隐私下保持了稳定的模型性能；多机构工作负载实验表明该架构在最小开销下保持效用，并提供持续的风险感知治理

Conclusion: 该框架为大规模部署可信且合规的分布式机器学习系统建立了实用基础

Abstract: Distributed machine learning systems require strong privacy guarantees, verifiable compliance, and scalable deployment across heterogeneous and multi-cloud environments. This work introduces a cloud-native privacy-preserving architecture that integrates federated learning, differential privacy, zero-knowledge compliance proofs, and adaptive governance powered by reinforcement learning. The framework supports secure model training and inference without centralizing sensitive data, while enabling cryptographically verifiable policy enforcement across institutions and cloud platforms. A full prototype deployed across hybrid Kubernetes clusters demonstrates reduced membership-inference risk, consistent enforcement of formal privacy budgets, and stable model performance under differential privacy. Experimental evaluation across multi-institution workloads shows that the architecture maintains utility with minimal overhead while providing continuous, risk-aware governance. The proposed framework establishes a practical foundation for deploying trustworthy and compliant distributed machine learning systems at scale.

</details>


### [45] [Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories](https://arxiv.org/abs/2512.10350)
*Nicolas Tacheny*

Main category: cs.LG

TL;DR: 论文提出几何框架分析智能体循环在语义嵌入空间中的动态行为，区分语言变换的artifact空间和几何测量的嵌入空间，通过等距校准消除余弦相似度偏差，识别收缩和发散两种基本动态机制。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体系统通过递归反馈循环运行，但人们对这些智能体循环的几何行为（收敛、发散或更复杂动态）了解甚少，缺乏系统分析框架。

Method: 引入语义嵌入空间的几何分析框架，将迭代变换视为离散动力系统；区分artifact空间和嵌入空间；提出等距校准方法消除余弦相似度的各向异性偏差；通过受控实验研究单一智能体循环。

Result: 识别出两种基本动态机制：收缩性重写循环收敛于稳定吸引子且分散度递减；探索性总结与否定循环产生无界发散且无聚类形成；两种机制展现出收缩和扩张的定性不同几何特征。

Conclusion: 提示设计直接控制智能体循环的动态机制，能够系统控制大语言模型迭代变换中的收敛、发散和轨迹结构，为智能体系统的几何行为分析提供了理论基础。

Abstract: Agentic systems built on large language models operate through recursive feedback loops, where each output becomes the next input. Yet the geometric behavior of these agentic loops (whether they converge, diverge, or exhibit more complex dynamics) remains poorly understood. This paper introduces a geometric framework for analyzing agentic trajectories in semantic embedding space, treating iterative transformations as discrete dynamical systems. We distinguish the artifact space, where linguistic transformations occur, from the embedding space, where geometric measurements are performed. Because cosine similarity is biased by embedding anisotropy, we introduce an isotonic calibration that eliminates systematic bias and aligns similarities with human semantic judgments while preserving high local stability. This enables rigorous measurement of trajectories, clusters and attractors. Through controlled experiments on singular agentic loops, we identify two fundamental regimes. A contractive rewriting loop converges toward a stable attractor with decreasing dispersion, while an exploratory summarize and negate loop produces unbounded divergence with no cluster formation. These regimes display qualitatively distinct geometric signatures of contraction and expansion. Our results show that prompt design directly governs the dynamical regime of an agentic loop, enabling systematic control of convergence, divergence and trajectory structure in iterative LLM transformations.

</details>


### [46] [GPG: Generalized Policy Gradient Theorem for Transformer-based Policies](https://arxiv.org/abs/2512.10365)
*Hangyu Mao,Guangting Dong,Zhicheng Dou*

Main category: cs.LG

TL;DR: 本文提出了专门为基于Transformer的策略设计的广义策略梯度(GPG)定理，证明了标准策略梯度定理和GRPO都是GPG的特殊情况，并探索了其在训练大语言模型中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有策略梯度方法在处理基于Transformer的策略时存在局限性，需要一种更通用的框架来统一不同的策略梯度方法，并更好地适应大语言模型的训练需求。

Method: 提出了广义策略梯度(GPG)定理，专门针对Transformer架构设计，该框架能够包含标准策略梯度定理和GRPO作为特例，为策略优化提供了更通用的理论基础。

Result: 成功建立了统一的策略梯度框架，证明了现有方法的特例关系，并为大语言模型的训练提供了新的优化视角和实用指导。

Conclusion: GPG定理为基于Transformer的策略优化提供了理论基础，统一了现有方法，为大语言模型的高效训练开辟了新途径。

Abstract: We present the Generalized Policy Gradient (GPG) Theorem, specifically designed for Transformer-based policies. Notably, we demonstrate that both standard Policy Gradient Theorem and GRPO emerge as special cases within our GPG framework. Furthermore, we explore its practical applications in training Large Language Models (LLMs), offering new insights into efficient policy optimization.

</details>


### [47] [Fitting magnetization data using continued fraction of straight lines](https://arxiv.org/abs/2512.10390)
*Vijay Prakash S*

Main category: cs.LG

TL;DR: 使用连分数直线组合近似铁磁材料的非线性磁化曲线，用于解释磁畴生长和收缩行为


<details>
  <summary>Details</summary>
Motivation: 铁磁材料在外加磁场作用下的磁化过程具有非线性特性，这种非线性源于微观磁畴与外加磁场的对齐过程。需要建立数学模型来描述这种非线性行为，特别是磁畴生长和收缩的动态过程。

Method: 将非线性磁化函数近似为直线的连分数组合，这种代数表达式可以通过非线性回归来估计参数，从而拟合磁化曲线。

Result: 提出的连分数直线组合方法能够有效近似铁磁材料的非线性磁化行为，为解释磁畴生长和收缩过程提供了数学工具。

Conclusion: 连分数直线组合是一种有效的数学工具，可用于描述铁磁材料的非线性磁化特性，特别是在分析磁畴动态行为方面具有应用价值。

Abstract: Magnetization of a ferromagnetic substance in response to an externally applied magnetic field increases with the strength of the field. This is because at the microscopic level, magnetic moments in certain regions or domains of the substance increasingly align with the applied field, while the amount of misaligned domains decreases. The alignment of such magnetic domains with an applied magnetic field forms the physical basis for the nonlinearity of magnetization. In this paper, the nonlinear function is approximated as a combination of continued fraction of straight lines. The resulting fit is used to interpret the nonlinear behavior in both growing and shrinking magnetic domains. The continued fraction of straight lines used here is an algebraic expression which can be used to estimate parameters using nonlinear regression.

</details>


### [48] [Metacognitive Sensitivity for Test-Time Dynamic Model Selection](https://arxiv.org/abs/2512.10451)
*Le Tuan Minh Trinh,Le Minh Vu Pham,Thi Minh Anh Pham,An Duc Nguyen*

Main category: cs.LG

TL;DR: 提出基于人类认知科学的AI元认知评估框架，使用meta-d'衡量模型置信度预测准确性，并基于此进行动态模型选择，提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然能表达预测置信度，但往往存在校准不佳的问题（置信度不能真实反映能力）。受人类认知科学启发，需要评估AI模型的元认知能力，即模型是否真正知道自己知道什么。

Method: 1. 引入心理学基础的meta-d'指标来衡量元认知敏感性，表征模型置信度预测自身准确性的可靠性；2. 使用动态敏感性分数作为上下文，构建基于bandit的仲裁器，在测试时学习选择信任哪个专家模型；3. 在多个数据集和深度学习模型组合（包括CNN和VLM）上进行实验验证。

Result: 实验表明，这种元认知方法在多个数据集和模型组合上都能提高联合推理准确性，优于各个组成模型。该方法将模型选择重新定义为评估短期信号（置信度预测分数）和中期特征（元认知敏感性）的问题。

Conclusion: 该工作为AI模型提供了新颖的行为描述框架，将集成选择重新定义为评估短期信号和中期特征的问题，通过元认知方法有效提升了模型推理性能。

Abstract: A key aspect of human cognition is metacognition - the ability to assess one's own knowledge and judgment reliability. While deep learning models can express confidence in their predictions, they often suffer from poor calibration, a cognitive bias where expressed confidence does not reflect true competence. Do models truly know what they know? Drawing from human cognitive science, we propose a new framework for evaluating and leveraging AI metacognition. We introduce meta-d', a psychologically-grounded measure of metacognitive sensitivity, to characterise how reliably a model's confidence predicts its own accuracy. We then use this dynamic sensitivity score as context for a bandit-based arbiter that performs test-time model selection, learning which of several expert models to trust for a given task. Our experiments across multiple datasets and deep learning model combinations (including CNNs and VLMs) demonstrate that this metacognitive approach improves joint-inference accuracy over constituent models. This work provides a novel behavioural account of AI models, recasting ensemble selection as a problem of evaluating both short-term signals (confidence prediction scores) and medium-term traits (metacognitive sensitivity).

</details>


### [49] [Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2512.10510)
*Chihyeon Song,Jaewoo Lee,Jinkyoo Park*

Main category: cs.LG

TL;DR: 提出自适应回放缓冲区（ARB），通过动态调整离线与在线数据采样权重来优化离线到在线强化学习，避免早期性能下降并提升最终性能


<details>
  <summary>Details</summary>
Motivation: 离线到在线强化学习面临平衡固定离线数据集与新收集在线经验的困境，现有方法通常依赖固定数据混合比例，难以兼顾早期学习稳定性和渐进性能

Method: 引入自适应回放缓冲区（ARB），基于轻量级指标"策略一致性"动态优先采样数据，无需复杂学习过程，可无缝集成到现有O2O RL算法中

Result: 在D4RL基准测试中，ARB能持续缓解早期性能下降，显著提升各种O2O RL算法的最终性能

Conclusion: 自适应、行为感知的回放缓冲区设计对于离线到在线强化学习至关重要，ARB提供了一种简单有效的解决方案

Abstract: Offline-to-Online Reinforcement Learning (O2O RL) faces a critical dilemma in balancing the use of a fixed offline dataset with newly collected online experiences. Standard methods, often relying on a fixed data-mixing ratio, struggle to manage the trade-off between early learning stability and asymptotic performance. To overcome this, we introduce the Adaptive Replay Buffer (ARB), a novel approach that dynamically prioritizes data sampling based on a lightweight metric we call 'on-policyness'. Unlike prior methods that rely on complex learning procedures or fixed ratios, ARB is designed to be learning-free and simple to implement, seamlessly integrating into existing O2O RL algorithms. It assesses how closely collected trajectories align with the current policy's behavior and assigns a proportional sampling weight to each transition within that trajectory. This strategy effectively leverages offline data for initial stability while progressively focusing learning on the most relevant, high-rewarding online experiences. Our extensive experiments on D4RL benchmarks demonstrate that ARB consistently mitigates early performance degradation and significantly improves the final performance of various O2O RL algorithms, highlighting the importance of an adaptive, behavior-aware replay buffer design.

</details>


### [50] [Disentangled and Distilled Encoder for Out-of-Distribution Reasoning with Rademacher Guarantees](https://arxiv.org/abs/2512.10522)
*Zahra Rahiminasab,Michael Yuhas,Arvind Easwaran*

Main category: cs.LG

TL;DR: 提出解耦蒸馏编码器(DDE)框架，通过师生蒸馏压缩模型，在资源受限设备上部署时保持解耦特性，用于多标签OOD样本推理


<details>
  <summary>Details</summary>
Motivation: 变分自编码器的解耦潜在空间可用于多标签OOD样本推理，但模型过大难以在资源受限设备上部署，需要压缩模型同时保持解耦特性

Method: 提出解耦蒸馏编码器(DDE)框架，将师生蒸馏形式化为带约束的优化问题，通过解耦约束保持解耦特性，基于Rademacher复杂度建立理论保证

Result: 在NVIDIA设备上部署压缩模型进行实证评估，实现了模型压缩同时保持解耦特性

Conclusion: DDE框架成功压缩了用于OOD推理的解耦模型，使其能在资源受限设备上部署，同时保持了重要的解耦特性

Abstract: Recently, the disentangled latent space of a variational autoencoder (VAE) has been used to reason about multi-label out-of-distribution (OOD) test samples that are derived from different distributions than training samples. Disentangled latent space means having one-to-many maps between latent dimensions and generative factors or important characteristics of an image. This paper proposes a disentangled distilled encoder (DDE) framework to decrease the OOD reasoner size for deployment on resource-constrained devices while preserving disentanglement. DDE formalizes student-teacher distillation for model compression as a constrained optimization problem while preserving disentanglement with disentanglement constraints. Theoretical guarantees for disentanglement during distillation based on Rademacher complexity are established. The approach is evaluated empirically by deploying the compressed model on an NVIDIA

</details>


### [51] [Mode-Seeking for Inverse Problems with Diffusion Models](https://arxiv.org/abs/2512.10524)
*Sai Bharath Chandra Gutha,Ricardo Vinuesa,Hossein Azizpour*

Main category: cs.LG

TL;DR: 提出VML-MAP算法，通过变分模式寻找损失引导扩散模型解决逆问题，无需任务特定训练，在计算效率和性能上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练无条件扩散模型的后验采样或MAP估计方法存在建模近似和计算效率低的问题，需要更高效准确的逆问题求解方法

Method: 提出变分模式寻找损失(VML)，通过最小化扩散后验与测量后验之间的KL散度来引导生成样本趋向MAP估计；对于线性逆问题，VML可解析推导而无需近似；基于此提出VML-MAP算法

Result: 在多个数据集上的多样化图像恢复任务中，VML-MAP在性能和计算时间上均优于现有方法

Conclusion: VML-MAP为基于扩散模型的逆问题求解提供了高效准确的解决方案，无需任务特定训练，具有理论保证和实际应用价值

Abstract: A pre-trained unconditional diffusion model, combined with posterior sampling or maximum a posteriori (MAP) estimation techniques, can solve arbitrary inverse problems without task-specific training or fine-tuning. However, existing posterior sampling and MAP estimation methods often rely on modeling approximations and can be computationally demanding. In this work, we propose the variational mode-seeking loss (VML), which, when minimized during each reverse diffusion step, guides the generated sample towards the MAP estimate. VML arises from a novel perspective of minimizing the Kullback-Leibler (KL) divergence between the diffusion posterior $p(\mathbf{x}_0|\mathbf{x}_t)$ and the measurement posterior $p(\mathbf{x}_0|\mathbf{y})$, where $\mathbf{y}$ denotes the measurement. Importantly, for linear inverse problems, VML can be analytically derived and need not be approximated. Based on further theoretical insights, we propose VML-MAP, an empirically effective algorithm for solving inverse problems, and validate its efficacy over existing methods in both performance and computational time, through extensive experiments on diverse image-restoration tasks across multiple datasets.

</details>


### [52] [Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders](https://arxiv.org/abs/2512.10547)
*Qingsen Ma,Dianyun Wang,Jiaming Lyu,Yaoye Wang,Lechen Ning,Sujie Zhu,Zhenbo Xu,Liuyu Xiang,Huining Li,Huijia Wu,Zhaofeng He*

Main category: cs.LG

TL;DR: STA-Attention框架使用Top-K稀疏自编码器将KV缓存分解为可解释的语义原子，通过双预算策略选择性保留信息量最大的语义成分，在保持模型性能的同时解决长上下文LLM中的KV缓存内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: KV缓存是长上下文大语言模型中的主要内存瓶颈，但通常被当作不透明的数值张量处理。作者希望将KV缓存分解为可解释的语义原子，以弥合机制可解释性与忠实注意力建模之间的差距。

Method: 提出STA-Attention框架，使用Top-K稀疏自编码器分解KV缓存，消除标准L1正则化SAE的收缩偏差，保持注意力所需的精确点积几何。基于发现的键值不对称性（键向量作为稀疏路由器，值向量携带密集内容），引入双预算策略选择性保留最有信息的语义成分。

Result: 在Yi-6B、Mistral-7B、Qwen2.5-32B等模型上的实验表明，语义重建保持了与原始模型相当的困惑度和零样本性能，有效解决了KV缓存的内存瓶颈问题。

Conclusion: STA-Attention成功将KV缓存分解为可解释的语义原子，揭示了键值不对称的基本结构，通过双预算策略在保持模型性能的同时有效管理内存使用，为长上下文LLM的内存优化提供了新方法。

Abstract: The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.

</details>


### [53] [Is the Information Bottleneck Robust Enough? Towards Label-Noise Resistant Information Bottleneck Learning](https://arxiv.org/abs/2512.10573)
*Yi Huang,Qingyun Sun,Yisen Gao,Haonan Yuan,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: LaT-IB是一种抗标签噪声的信息瓶颈方法，通过引入"最小充分干净"准则和噪声感知潜在解耦，在保留任务相关信息的同时丢弃噪声，解决了标准IB方法对标签噪声的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 信息瓶颈(IB)原理在表示学习中很有效，但它严重依赖准确的标签，在现实场景中容易受到标签噪声的影响，导致性能下降和过拟合。需要一种能够抵抗标签噪声的IB方法。

Method: 提出LaT-IB方法，引入"最小充分干净"(MSC)准则作为互信息正则化器。采用噪声感知潜在解耦，将潜在表示分解为与干净标签空间和噪声空间对齐的组件。设计三阶段训练框架：预热、知识注入和鲁棒训练。

Result: 大量实验表明，LaT-IB在标签噪声下实现了优越的鲁棒性和效率，显著增强了在现实标签噪声场景中的鲁棒性和适用性。

Conclusion: LaT-IB通过MSC准则和噪声感知解耦，有效解决了信息瓶颈对标签噪声的脆弱性问题，为现实世界中的噪声标签场景提供了实用的解决方案。

Abstract: The Information Bottleneck (IB) principle facilitates effective representation learning by preserving label-relevant information while compressing irrelevant information. However, its strong reliance on accurate labels makes it inherently vulnerable to label noise, prevalent in real-world scenarios, resulting in significant performance degradation and overfitting. To address this issue, we propose LaT-IB, a novel Label-Noise ResistanT Information Bottleneck method which introduces a "Minimal-Sufficient-Clean" (MSC) criterion. Instantiated as a mutual information regularizer to retain task-relevant information while discarding noise, MSC addresses standard IB's vulnerability to noisy label supervision. To achieve this, LaT-IB employs a noise-aware latent disentanglement that decomposes the latent representation into components aligned with to the clean label space and the noise space. Theoretically, we first derive mutual information bounds for each component of our objective including prediction, compression, and disentanglement, and moreover prove that optimizing it encourages representations invariant to input noise and separates clean and noisy label information. Furthermore, we design a three-phase training framework: Warmup, Knowledge Injection and Robust Training, to progressively guide the model toward noise-resistant representations. Extensive experiments demonstrate that LaT-IB achieves superior robustness and efficiency under label noise, significantly enhancing robustness and applicability in real-world scenarios with label noise.

</details>


### [54] [Multi-Objective Reward and Preference Optimization: Theory and Algorithms](https://arxiv.org/abs/2512.10601)
*Akhil Agnihotri*

Main category: cs.LG

TL;DR: 该论文在约束强化学习领域提出了一系列理论框架和算法，涵盖平均成本CMDP、有限时域CMDP、人类偏好学习以及大语言模型对齐，提供了具有理论保证的实用工具。


<details>
  <summary>Details</summary>
Motivation: 推动约束强化学习在控制、偏好学习和大型语言模型对齐方面的理论发展，解决安全关键环境中的决策问题，实现安全和对齐的智能系统。

Method: 1) ACPO算法：结合敏感性分析和信任域更新处理平均成本CMDP；2) e-COP算法：基于时域策略差异引理处理有限时域CMDP；3) warmPref-PS：整合异构评分者离线偏好数据的后验采样策略；4) PSPL算法：从轨迹比较中联合采样奖励模型和转移动态；5) MOPO算法：多目标约束优化方法，适用于大规模语言模型对齐。

Result: ACPO在平均成本CMDP中达到最先进的实证性能；e-COP在安全关键环境中提供可证明的性能和可扩展性；warmPref-PS显著减少遗憾并提高数据收集效率；PSPL提供贝叶斯简单遗憾保证并稳健识别最优策略；MOPO可扩展到数十亿参数的语言模型并在各种对齐设置中保持稳健。

Conclusion: 该论文统一了平均成本、时域和偏好驱动的约束强化学习范式，为安全和对齐的决策提供了理论进展和实用工具，推动了约束RL在多个应用领域的实际部署。

Abstract: This thesis develops theoretical frameworks and algorithms that advance constrained reinforcement learning (RL) across control, preference learning, and alignment of large language models. The first contribution addresses constrained Markov Decision Processes (CMDPs) under the average-cost criterion through the Average-Constrained Policy Optimization (ACPO) algorithm. ACPO integrates sensitivity analysis with trust-region updates to ensure stable constraint handling, achieving state-of-the-art empirical performance with theoretical guarantees. Constrained RL is then extended to finite-horizon settings via e-COP, the first policy optimization method for episodic CMDPs. Built on an episodic policy difference lemma, e-COP offers provable performance, simplicity, and scalability in safety-critical environments. The thesis then investigates reinforcement learning from human preferences. warmPref-PS introduces a posterior sampling strategy for linear bandits that integrates offline preference data from heterogeneous raters into online learning. Explicit modeling of rater competence yields substantial regret reduction and more efficient data collection for RLHF. The PSPL algorithm further advances preference-based RL by jointly sampling reward models and transition dynamics from pairwise trajectory comparisons, providing Bayesian simple-regret guarantees and robust empirical identification of optimal policies. The final contribution applies these methods to large-scale model alignment. A multi-objective constrained optimization view yields MOPO, an iterative algorithm with closed-form updates that scales to multi-billion-parameter language models and remains robust across alignment settings. Collectively, the thesis unifies constrained RL across average-cost, episodic, and preference-driven paradigms, delivering theoretical advances and practical tools for safe and aligned decision-making.

</details>


### [55] [Supporting Migration Policies with Forecasts: Illegal Border Crossings in Europe through a Mixed Approach](https://arxiv.org/abs/2512.10633)
*C. Bosco,U. Minora,D. de Rigo,J. Pingsdorf,R. Cortinovis*

Main category: cs.LG

TL;DR: 提出一种混合方法预测欧洲非法越境，结合机器学习与专家定性分析，为欧盟移民政策提供一年期预测支持


<details>
  <summary>Details</summary>
Motivation: 应对移民模式突变和传统数据局限性的挑战，满足《欧盟移民与庇护公约》的预测需求，支持欧盟成员国间的战略决策和团结机制

Method: 混合方法整合机器学习技术与移民专家定性见解，创新性地引入人工评估协变量，提高数据驱动模型的预测能力

Result: 方法经过已知数据测试验证，证明其在移民政策背景下的适用性和可靠性，为欧盟移民治理提供新型操作工具

Conclusion: 数据驱动建模与专家判断相结合的方法符合学术建议，为欧盟移民政策提供政策相关的预测，支持预警系统和战略决策

Abstract: This paper presents a mixed-methodology to forecast illegal border crossings in Europe across five key migratory routes, with a one-year time horizon. The methodology integrates machine learning techniques with qualitative insights from migration experts. This approach aims at improving the predictive capacity of data-driven models through the inclusion of a human-assessed covariate, an innovation that addresses challenges posed by sudden shifts in migration patterns and limitations in traditional datasets. The proposed methodology responds directly to the forecasting needs outlined in the EU Pact on Migration and Asylum, supporting the Asylum and Migration Management Regulation (AMMR). It is designed to provide policy-relevant forecasts that inform strategic decisions, early warning systems, and solidarity mechanisms among EU Member States. By joining data-driven modeling with expert judgment, this work aligns with existing academic recommendations and introduces a novel operational tool tailored for EU migration governance. The methodology is tested and validated with known data to demonstrate its applicability and reliability in migration-related policy context.

</details>


### [56] [Token Sample Complexity of Attention](https://arxiv.org/abs/2512.10656)
*Léa Bohbot,Cyril Letrouit,Gabriel Peyré,François-Xavier Vialard*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型中注意力机制在极端序列长度下的收敛行为，提出了token-sample复杂度的概念，分析了注意力映射在不同条件下的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型上下文窗口的不断扩大，需要研究注意力机制在极端序列长度下的行为特征。本文旨在量化注意力计算在有限token数下收敛到无限token极限的速率。

Method: 引入token-sample复杂度概念，从两个层面估计有限n的收敛边界：1）注意力映射的点态一致收敛；2）变换后token分布矩的收敛。针对紧支撑分布和亚高斯分布，分析不同几何和谱特性下的收敛速率。

Result: 对于紧支撑分布，注意力映射在半径为R的球上以C(R)/√n的速率一致收敛，其中C(R)随R指数增长。对于变换分布矩的收敛，速率为C'(R)/n^β，β<1/2，C'(R)与分布支撑大小多项式相关。在注意力参数趋于无穷时，收敛速率为对数级。

Conclusion: 本文建立了注意力机制在不同条件下的收敛理论框架，为理解大型语言模型在长序列下的行为提供了理论基础，实验验证了理论预测的有效性。

Abstract: As context windows in large language models continue to expand, it is essential to characterize how attention behaves at extreme sequence lengths. We introduce token-sample complexity: the rate at which attention computed on $n$ tokens converges to its infinite-token limit. We estimate finite-$n$ convergence bounds at two levels: pointwise uniform convergence of the attention map, and convergence of moments for the transformed token distribution. For compactly supported (and more generally sub-Gaussian) distributions, our first result shows that the attention map converges uniformly on a ball of radius $R$ at rate $C(R)/\sqrt{n}$, where $C(R)$ grows exponentially with $R$. For large $R$, this estimate loses practical value, and our second result addresses this issue by establishing convergence rates for the moments of the transformed distribution (the token output of the attention layer). In this case, the rate is $C'(R)/n^β$ with $β<\tfrac{1}{2}$, and $C'(R)$ depends polynomially on the size of the support of the distribution. The exponent $β$ depends on the attention geometry and the spectral properties of the tokens distribution. We also examine the regime in which the attention parameter tends to infinity and the softmax approaches a hardmax, and in this setting, we establish a logarithmic rate of convergence. Experiments on synthetic Gaussian data and real BERT models on Wikipedia text confirm our predictions.

</details>


### [57] [DCFO Additional Material](https://arxiv.org/abs/2512.10659)
*Tommaso Amico,Pernille Matthews,Lena Krieger,Arthur Zimek,Ira Assent*

Main category: cs.LG

TL;DR: 提出DCFO方法为LOF异常检测算法生成反事实解释，通过数据空间分区实现梯度优化，在50个数据集上验证优于基准方法


<details>
  <summary>Details</summary>
Motivation: 异常检测需要可解释性，现有反事实解释方法未能针对LOF等经典异常检测算法，LOF作为最流行的无监督异常检测方法缺乏可解释性

Method: DCFO方法将数据空间划分为LOF行为平滑的区域，实现高效的基于梯度的优化，专门为LOF生成反事实解释

Result: 在50个OpenML数据集上的实验验证表明，DCFO在生成反事实的邻近性和有效性方面持续优于基准竞争对手

Conclusion: DCFO成功解决了LOF异常检测算法的可解释性问题，为经典异常检测算法提供了有效的反事实解释方法

Abstract: Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.

</details>


### [58] [Learning by Analogy: A Causal Framework for Composition Generalization](https://arxiv.org/abs/2512.10669)
*Lingjing Kong,Shaoan Xie,Yang Jiao,Yetian Chen,Yanhui Guo,Simone Shao,Yan Gao,Guangyi Chen,Kun Zhang*

Main category: cs.LG

TL;DR: 论文提出基于因果模块化和最小变化原则的形式化框架，通过分层数据生成过程实现组合泛化，证明潜在分层结构可从观测数据中识别，并在基准数据集上验证了理论有效性。


<details>
  <summary>Details</summary>
Motivation: 组合泛化能力使模型能够超越有限经验理解和生成新概念组合，但目前支持这一关键能力的数据结构和原理尚不明确。研究者认为组合泛化需要将高级概念分解为可跨相似上下文重组的低级基本概念，类似于人类通过类比推理的方式。

Method: 采用因果模块化和最小变化原则形式化组合泛化过程，引入分层数据生成过程来编码不同层次概念及其交互机制。理论证明该方法支持复杂概念关系的组合泛化，并证明潜在分层结构可从文本-图像对等观测数据中可识别地恢复。

Result: 理论框架能够支持复杂概念关系的组合泛化，超越先前仅假设简单交互（如加性效应）的工作。在基准数据集上应用理论见解实现了显著性能提升。

Conclusion: 通过因果模块化和分层数据生成过程的形式化框架，为组合泛化提供了理论基础，证明潜在结构可从观测数据中恢复，并在实践中验证了理论的有效性，为理解和实现组合泛化能力提供了新途径。

Abstract: Compositional generalization -- the ability to understand and generate novel combinations of learned concepts -- enables models to extend their capabilities beyond limited experiences. While effective, the data structures and principles that enable this crucial capability remain poorly understood. We propose that compositional generalization fundamentally requires decomposing high-level concepts into basic, low-level concepts that can be recombined across similar contexts, similar to how humans draw analogies between concepts. For example, someone who has never seen a peacock eating rice can envision this scene by relating it to their previous observations of a chicken eating rice.
  In this work, we formalize these intuitive processes using principles of causal modularity and minimal changes. We introduce a hierarchical data-generating process that naturally encodes different levels of concepts and their interaction mechanisms. Theoretically, we demonstrate that this approach enables compositional generalization supporting complex relations between composed concepts, advancing beyond prior work that assumes simpler interactions like additive effects. Critically, we also prove that this latent hierarchical structure is provably recoverable (identifiable) from observable data like text-image pairs, a necessary step for learning such a generative process. To validate our theory, we apply insights from our theoretical framework and achieve significant improvements on benchmark datasets.

</details>


### [59] [HybridVFL: Disentangled Feature Learning for Edge-Enabled Vertical Federated Multimodal Classification](https://arxiv.org/abs/2512.10701)
*Mostafa Anoosha,Zeinab Dehghani,Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker*

Main category: cs.LG

TL;DR: HybridVFL框架通过客户端特征解耦和服务器端跨模态Transformer融合，在垂直联邦学习中显著提升了多模态数据融合性能


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习在边缘AI场景（如移动健康诊断）中提供隐私保护，但标准VFL系统由于简单的特征融合而存在性能限制

Method: 提出HybridVFL框架，采用客户端特征解耦和服务器端跨模态Transformer进行上下文感知融合

Result: 在多模态HAM10000皮肤病变数据集上，HybridVFL显著优于标准联邦学习基线

Conclusion: 验证了先进融合机制在鲁棒、隐私保护系统中的关键性

Abstract: Vertical Federated Learning (VFL) offers a privacy-preserving paradigm for Edge AI scenarios like mobile health diagnostics, where sensitive multimodal data reside on distributed, resource-constrained devices. Yet, standard VFL systems often suffer performance limitations due to simplistic feature fusion. This paper introduces HybridVFL, a novel framework designed to overcome this bottleneck by employing client-side feature disentanglement paired with a server-side cross-modal transformer for context-aware fusion. Through systematic evaluation on the multimodal HAM10000 skin lesion dataset, we demonstrate that HybridVFL significantly outperforms standard federated baselines, validating the criticality of advanced fusion mechanisms in robust, privacy-preserving systems.

</details>


### [60] [Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality](https://arxiv.org/abs/2512.10720)
*Lingjing Kong,Shaoan Xie,Guangyi Chen,Yuewen Sun,Xiangchen Song,Eric P. Xing,Kun Zhang*

Main category: cs.LG

TL;DR: 论文提出基于因果最小化原则的理论框架，为深度生成模型提供可解释性基础，使潜在表示具有因果解释和组件级可控性。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型虽然革命性地改变了图像和文本生成领域，但大多作为不透明的黑箱运行，阻碍了人类的理解、控制和对齐。现有方法如稀疏自编码器（SAEs）虽然经验上成功，但缺乏理论保证，可能导致主观见解。

Method: 引入基于因果最小化原则的理论框架——层次选择模型，其中高层概念由低层变量的约束组合产生。在理论推导的最小化条件（表现为稀疏性或压缩约束）下，学习到的表示可以等价于数据生成过程的真实潜在变量。

Result: 将约束应用于领先的生成模型（扩散视觉和自回归语言模型），能够提取其固有的层次概念图，揭示内部知识组织的新见解。这些因果基础的概念可作为细粒度模型控制的杠杆。

Conclusion: 因果最小化原则为可解释生成模型提供了原则性基础，使潜在表示具有清晰的因果解释和鲁棒的组件级可识别控制，为实现透明可靠的系统铺平道路。

Abstract: Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems.

</details>


### [61] [Generalized Spherical Neural Operators: Green's Function Formulation](https://arxiv.org/abs/2512.10723)
*Hao Tang,Hao Chen,Chao Li*

Main category: cs.LG

TL;DR: 提出基于可设计球面格林函数及其谐波展开的通用算子设计框架，建立球面学习的算子理论基础，开发GSNO算子和GSHNet架构，在多个球面应用领域超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 神经算子虽能有效求解参数偏微分方程，但扩展到球面域仍具挑战性，需在保持内在几何结构的同时避免破坏旋转一致性的失真。现有球面算子依赖旋转等变性但缺乏处理现实世界复杂性的灵活性。

Method: 基于可设计球面格林函数及其谐波展开建立算子理论基础，提出绝对和相对位置依赖的格林函数，平衡等变性和不变性。开发GSNO算子结合新颖谱学习方法，构建GSHNet分层架构，结合多尺度谱建模与球面上下采样。

Result: 在扩散MRI、浅水动力学和全球天气预报等任务中，GSNO和GSHNet一致优于最先进方法，证明了其在各向异性、约束丰富系统中的适应性和谱效率。

Conclusion: GSNO为球面算子学习提供了一个原理性通用框架，将严谨理论与现实世界复杂性相连接，为球面域上的参数偏微分方程求解提供了新的有效途径。

Abstract: Neural operators offer powerful approaches for solving parametric partial differential equations, but extending them to spherical domains remains challenging due to the need to preserve intrinsic geometry while avoiding distortions that break rotational consistency. Existing spherical operators rely on rotational equivariance but often lack the flexibility for real-world complexity. We propose a general operator-design framework based on the designable spherical Green's function and its harmonic expansion, establishing a solid operator-theoretic foundation for spherical learning. Based on this, we propose an absolute and relative position-dependent Green's function that enables flexible balance of equivariance and invariance for real-world modeling. The resulting operator, Green's-function Spherical Neural Operator (GSNO) with a novel spectral learning method, can adapt to anisotropic, constraint-rich systems while retaining spectral efficiency. To exploit GSNO, we develop GSHNet, a hierarchical architecture that combines multi-scale spectral modeling with spherical up-down sampling, enhancing global feature representation. Evaluations on diffusion MRI, shallow water dynamics, and global weather forecasting, GSNO and GSHNet consistently outperform state-of-the-art methods. Our results position GSNO as a principled and general framework for spherical operator learning, bridging rigorous theory with real-world complexity.

</details>


### [62] [Interpretable and Steerable Concept Bottleneck Sparse Autoencoders](https://arxiv.org/abs/2512.10805)
*Akshay Kulkarni,Tsui-Wei Weng,Vivek Narayanaswamy,Shusen Liu,Wesam A. Sakla,Kowshik Thopalli*

Main category: cs.LG

TL;DR: 论文提出CB-SAE框架，通过剪枝低效用神经元并添加轻量级概念瓶颈，提升稀疏自编码器的可解释性和可操控性。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在大型语言模型和视觉语言模型中具有巨大潜力，但现有方法存在两个主要问题：1）大多数神经元要么可解释性低，要么可操控性低，要么两者都低；2）由于无监督学习的性质，用户期望的概念往往不在学习到的字典中，限制了实际应用。

Method: 提出概念瓶颈稀疏自编码器（CB-SAE）框架，包含两个关键步骤：1）剪枝低效用神经元；2）在潜在空间中添加轻量级概念瓶颈，与用户定义的概念集对齐。同时引入了两个计算成本低的可解释性和可操控性度量指标。

Result: CB-SAE在视觉语言模型和图像生成任务中，将可解释性提升了32.1%，可操控性提升了14.5%。

Conclusion: CB-SAE框架有效解决了稀疏自编码器在可解释性和可操控性方面的局限性，为模型的可解释性、概念发现和模型操控提供了更实用的解决方案。

Abstract: Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.

</details>


### [63] [Learning Controllable and Diverse Player Behaviors in Multi-Agent Environments](https://arxiv.org/abs/2512.10835)
*Atahan Cilan,Atay Özgövde*

Main category: cs.LG

TL;DR: 提出强化学习框架，无需人类游戏数据即可生成可控多样的玩家行为，通过行为向量空间和距离奖励实现行为控制


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量玩家轨迹数据、为不同玩家类型训练单独模型，或缺乏可解释行为参数与学习策略的直接映射，限制了可扩展性和可控性

Method: 在N维连续空间中定义玩家行为，从包含真实人类风格子集的区域均匀采样目标行为向量；训练时每个智能体接收当前和目标行为向量作为输入，奖励基于两者距离的归一化减少；使用PPO-based多智能体策略

Result: 在自定义多玩家Unity游戏中，相比仅以胜利为目标的基线，该方法产生显著更大的行为多样性，并能可靠匹配指定的行为向量；单个策略可重现新的或未见过的游戏风格而无需重新训练

Conclusion: 该方法为自动化游戏测试、游戏平衡、人类行为模拟和在线游戏中替换断开连接的玩家提供了可扩展的解决方案

Abstract: This paper introduces a reinforcement learning framework that enables controllable and diverse player behaviors without relying on human gameplay data. Existing approaches often require large-scale player trajectories, train separate models for different player types, or provide no direct mapping between interpretable behavioral parameters and the learned policy, limiting their scalability and controllability. We define player behavior in an N-dimensional continuous space and uniformly sample target behavior vectors from a region that encompasses the subset representing real human styles. During training, each agent receives both its current and target behavior vectors as input, and the reward is based on the normalized reduction in distance between them. This allows the policy to learn how actions influence behavioral statistics, enabling smooth control over attributes such as aggressiveness, mobility, and cooperativeness. A single PPO-based multi-agent policy can reproduce new or unseen play styles without retraining. Experiments conducted in a custom multi-player Unity game show that the proposed framework produces significantly greater behavioral diversity than a win-only baseline and reliably matches specified behavior vectors across diverse targets. The method offers a scalable solution for automated playtesting, game balancing, human-like behavior simulation, and replacing disconnected players in online games.

</details>


### [64] [Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic Interpolants](https://arxiv.org/abs/2512.10857)
*Chirag Modi,Jiequn Han,Eric Vanden-Eijnden,Joan Bruna*

Main category: cs.LG

TL;DR: 提出一种基于随机插值的自洽方法（SCSI），用于从噪声观测数据中学习干净数据的生成模型，通过迭代更新损坏数据与干净数据之间的传输映射来反演损坏通道。


<details>
  <summary>Details</summary>
Motivation: 在科学和工程领域，通常只能获得通过噪声、病态通道损坏的观测数据，而无法获得干净数据。需要一种能够从损坏数据中学习干净数据生成模型的方法，这需要在分布层面解决逆问题。

Method: 基于随机插值方法，迭代更新损坏数据与干净数据样本之间的传输映射。该方法仅需访问损坏数据集以及对损坏通道的黑盒访问，通过自洽迭代过程收敛到能够有效反演损坏通道的传输映射。

Result: SCSI方法在计算效率上优于变分替代方法，具有高度灵活性（可处理任意非线性前向模型），并享有理论保证。在自然图像处理和科学重建的逆问题上表现出优越性能。

Conclusion: 自洽随机插值（SCSI）提供了一种有效的方法，能够从损坏数据中学习干净数据的生成模型，具有计算效率高、灵活性强和理论保证等优点，适用于各种逆问题应用。

Abstract: Transport-based methods have emerged as a leading paradigm for building generative models from large, clean datasets. However, in many scientific and engineering domains, clean data are often unavailable: instead, we only observe measurements corrupted through a noisy, ill-conditioned channel. A generative model for the original data thus requires solving an inverse problem at the level of distributions. In this work, we introduce a novel approach to this task based on Stochastic Interpolants: we iteratively update a transport map between corrupted and clean data samples using only access to the corrupted dataset as well as black box access to the corruption channel. Under appropriate conditions, this iterative procedure converges towards a self-consistent transport map that effectively inverts the corruption channel, thus enabling a generative model for the clean data. We refer to the resulting method as the self-consistent stochastic interpolant (SCSI). It (i) is computationally efficient compared to variational alternatives, (ii) highly flexible, handling arbitrary nonlinear forward models with only black-box access, and (iii) enjoys theoretical guarantees. We demonstrate superior performance on inverse problems in natural image processing and scientific reconstruction, and establish convergence guarantees of the scheme under appropriate assumptions.

</details>


### [65] [Scaling Behavior of Discrete Diffusion Language Models](https://arxiv.org/abs/2512.10858)
*Dimitri von Rütte,Janis Fluri,Omead Pooladzandi,Bernhard Schölkopf,Thomas Hofmann,Antonio Orvieto*

Main category: cs.LG

TL;DR: 本文研究了离散扩散语言模型（DLMs）在不同噪声类型下的缩放规律，发现均匀扩散比掩码扩散在参数效率上更高，更适合数据受限场景，并成功将均匀扩散模型扩展到100亿参数。


<details>
  <summary>Details</summary>
Motivation: 现代LLM预训练消耗大量计算资源和训练数据，不同模型的缩放规律成为关键区分因素。离散扩散语言模型作为自回归语言模型的替代方案，其缩放行为尚未被充分探索，先前研究表明DLMs需要更多数据和计算才能达到ALMs的性能。

Method: 通过平滑插值掩码扩散和均匀扩散来研究不同噪声类型下的DLMs缩放行为，并密切关注批量大小和学习率等关键超参数。实验分析了不同噪声类型在计算边界和数据边界下的表现差异。

Result: 研究发现DLMs的缩放行为强烈依赖于噪声类型，且与ALMs显著不同。在计算边界缩放中，所有噪声类型收敛到相似的损失值；但在计算效率训练中，均匀扩散需要更多参数和更少数据，而掩码扩散需要更多数据和更少参数。成功将均匀扩散模型扩展到100亿参数，训练计算量达10^22 FLOPs。

Conclusion: 均匀扩散在数据受限场景中表现出色，是DLMs的有前景候选方案。研究证实了预测的缩放规律，并创建了迄今为止已知最大的公开均匀扩散模型。

Abstract: Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.
  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for $10^{22}$ FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.

</details>


### [66] [UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting](https://arxiv.org/abs/2512.10866)
*Ruslan Gokhman*

Main category: cs.LG

TL;DR: 线性模型在仅使用历史温度数据的长期预测中表现优于复杂的Transformer架构，DLinear模型获得最佳精度


<details>
  <summary>Details</summary>
Motivation: 研究在仅使用历史温度数据（外生变量）的挑战性单变量设置下，比较线性模型和Transformer家族模型在长期温度预测中的表现

Method: 使用标准化训练、验证和测试分割，评估Linear、NLinear、DLinear、Transformer、Informer和Autoformer模型在长期外生温度预测任务中的性能

Result: 线性基线模型（Linear、NLinear、DLinear）始终优于更复杂的Transformer架构，其中DLinear在所有分割中实现了最佳整体精度

Conclusion: 精心设计的线性模型在仅使用外生变量的挑战性时间序列预测场景中仍然是强大的基线方法，复杂模型不一定带来性能提升

Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.

</details>


### [67] [Guided Transfer Learning for Discrete Diffusion Models](https://arxiv.org/abs/2512.10877)
*Julian Kleutgens,Claudio Battiloro,Lingkai Kong,Benjamin Grewe,Francesca Dominici,Mauricio Tec*

Main category: cs.LG

TL;DR: 提出GTL方法，使离散扩散模型无需微调即可适应新领域，并通过高效采样器解决大词汇量长序列的计算问题


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在语言等离散领域表现优异，但需要大量训练数据，而获取新领域数据成本高或风险大。现有迁移学习方法需要微调大型扩散模型，计算成本高且不实用。

Method: 基于连续扩散的比率迁移学习，提出离散扩散模型的引导迁移学习(GTL)，无需修改预训练去噪器即可从目标分布采样。同时提出高效引导采样器，通过规划选择位置和候选词来减少计算。

Result: GTL在序列数据（包括合成马尔可夫链和语言建模）上进行了评估，提供了其行为的实证分析。高效采样器使大规模语言建模在大词汇量和长序列上变得实用。

Conclusion: GTL为离散扩散模型提供了一种无需微调的迁移学习方法，通过高效采样器解决了计算瓶颈，使引导语言建模在大规模应用中变得实用。

Abstract: Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.

</details>


### [68] [SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale](https://arxiv.org/abs/2512.10922)
*Max Zimmer,Christophe Roux,Moritz Wagner,Deborah Hendrych,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型的高效剪枝方法，通过行级等稀疏度约束和1-swap优化算法，显著降低了剪枝误差并提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的全量重训练成本过高，传统全局幅度剪枝方法在Transformer架构上效果不佳，现有方法依赖近似或启发式算法，无法精确求解剪枝掩码选择问题。

Method: 提出行级等稀疏度约束，将问题解耦为行级优化；推导出基于校准数据Gram矩阵的高效最优1-swap计算方法；设计简单高效的1-swap算法，可从任意剪枝掩码开始，在GPU上高效运行且无需超参数调优。

Result: 相比Wanda方法，将每层剪枝误差降低高达60%；在多种GPT架构上一致提升了困惑度和零样本准确率；算法在大语言模型规模下具有高计算效率。

Conclusion: 通过行级约束和1-swap优化，成功将剪枝掩码选择问题变得在大语言模型规模下可高效求解，显著提升了剪枝效果和模型性能。

Abstract: The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.

</details>


### [69] [Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation](https://arxiv.org/abs/2512.10925)
*Zamirddine Mari,Mohamad Motasem Nawaf,Pierre Drap*

Main category: cs.LG

TL;DR: 本文提出了一种基于PPO算法的深度强化学习方法，用于解决水下机器人（BlueROV2）在无GPS、能见度差、存在障碍物环境中的自主导航问题，该方法在复杂环境中优于传统的DWA规划器，并成功实现了从仿真到真实环境的迁移。


<details>
  <summary>Details</summary>
Motivation: 水下环境自主导航面临三大挑战：GPS信号缺失、能见度降低以及水下障碍物存在。这些因素使得传统导航方法效果有限，需要开发更智能的导航策略来应对复杂的水下环境。

Method: 采用基于PPO（近端策略优化）算法的深度强化学习方法，构建包含目标导向导航信息、虚拟占用网格和操作区域边界射线投射的观测空间。在仿真环境中训练策略，并通过3D数字孪生技术监督真实BlueROV2进行验证。

Result: PPO策略在高度复杂环境中持续优于DWA（动态窗口法）基准方法，表现出更好的局部适应性和更少的碰撞。实验证实了学习行为从仿真到真实环境的可迁移性。

Conclusion: 深度强化学习方法在水下机器人自主导航中具有显著优势，特别是在复杂障碍物环境中。该方法通过仿真训练和数字孪生验证的结合，有效降低了真实世界实验的风险，证明了深度RL在水下机器人导航中的实用价值。

Abstract: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.

</details>


### [70] [Decoupled Q-Chunking](https://arxiv.org/abs/2512.10926)
*Qiyang Li,Seohong Park,Sergey Levine*

Main category: cs.LG

TL;DR: 本文提出了一种新算法，通过解耦评论家的块长度与策略的块长度，让策略在较短的动作块上操作，从而解决块状评论家方法中策略提取的挑战。


<details>
  <summary>Details</summary>
Motivation: 时间差分方法通过自举机制高效学习状态和动作价值，但容易产生自举偏差。块状评论家通过估计短动作序列的价值来加速价值备份，但从中提取策略存在挑战：策略必须以开环方式输出整个动作块，这在需要策略反应性的环境中可能次优，且随着块长度增长建模困难。

Method: 提出一种新算法，将评论家的块长度与策略的块长度解耦，允许策略在较短的动作块上操作。通过从原始块状评论家乐观地回溯，构建针对部分动作块的蒸馏评论家，近似估计部分动作块扩展为完整块时可实现的最大价值。

Result: 在具有挑战性的长时域离线目标条件任务上评估该方法，结果显示它可靠地优于先前的方法。

Conclusion: 该方法保留了多步价值传播的优势，同时避免了开环次优性和学习长动作块策略的困难，为块状评论家方法中的策略提取问题提供了有效解决方案。

Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.

</details>


### [71] [Asynchronous Reasoning: Training-Free Interactive Thinking LLMs](https://arxiv.org/abs/2512.10931)
*George Yakushev,Nataliia Babina,Masoud Vahid Dastgerdi,Vyacheslav Zhdanovskiy,Alina Shutova,Denis Kuznedelev*

Main category: cs.LG

TL;DR: 提出一种无需额外训练的方法，让具备推理能力的LLM能够像人类一样异步地思考、倾听和生成输出，显著减少实时交互延迟


<details>
  <summary>Details</summary>
Motivation: 现有LLM需要先完成思考才能响应，这在语音助手等实时交互场景中存在延迟问题，而人类可以异步地边听边思考边回答

Method: 利用旋转嵌入的特性，使原本设计用于顺序交互的LLM能够同时进行思考、倾听和输出生成

Result: 在数学、常识和安全推理任务上，方法能实时生成准确的思考增强答案，将首个非思考token的生成时间从分钟级降至≤5秒，整体实时延迟减少6-11倍

Conclusion: 通过旋转嵌入技术实现了LLM的异步推理能力，显著提升了实时交互性能，为语音助手等应用提供了更好的解决方案

Abstract: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.

</details>


### [72] [Stronger Normalization-Free Transformers](https://arxiv.org/abs/2512.10938)
*Mingzhi Chen,Taiming Lu,Jiachen Zhu,Mingjie Sun,Zhuang Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的点状函数Derf，作为归一化层的替代方案，在多种任务中超越了LayerNorm、RMSNorm和DyT的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管归一化层长期以来被视为深度学习架构中不可或缺的组件，但最近的Dynamic Tanh（DyT）表明替代方案是可能的。本研究旨在寻找比DyT更有效的函数设计，以进一步提升性能。

Method: 首先研究点状函数的内在特性如何影响训练和性能，然后基于这些发现进行大规模搜索，最终提出Derf函数：Derf(x) = erf(αx + s)，其中erf(x)是重新缩放的高斯累积分布函数。

Result: Derf在多个领域超越了LayerNorm、RMSNorm和DyT，包括视觉（图像识别和生成）、语音表示和DNA序列建模。性能提升主要源于改进的泛化能力而非更强的拟合能力。

Conclusion: Derf的简单性和更强的性能使其成为无归一化Transformer架构的实用选择，为深度学习架构设计提供了新的方向。

Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(αx + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.

</details>


### [73] [Hierarchical Dataset Selection for High-Quality Data Sharing](https://arxiv.org/abs/2512.10952)
*Xiaona Zhou,Yingyan Zeng,Ran Jin,Ismini Lourentzou*

Main category: cs.LG

TL;DR: DaSH方法通过层次化建模数据集和组级效用，在资源约束下从异构数据池中选择整个数据集以提升下游性能，相比现有方法准确率提升高达26.2%


<details>
  <summary>Details</summary>
Motivation: 现实世界中数据通常以离散数据集形式组织，不同数据集在相关性、质量和效用上存在差异。现有方法通常选择单个样本且将所有数据视为同等相关，忽略了数据集及其来源之间的差异，因此需要一种能够选择整个数据集的方法来改善下游性能

Method: 提出DaSH（Dataset Selection via Hierarchies）方法，在数据集和组（如集合、机构）两个层次上建模效用，通过层次化建模实现从有限观察中的高效泛化

Result: 在两个公共基准测试（Digit-Five和DomainNet）上，DaSH比最先进的数据选择基线方法准确率提升高达26.2%，同时需要显著更少的探索步骤。消融实验表明DaSH对低资源设置和相关数据集缺乏的情况具有鲁棒性

Conclusion: DaSH方法适用于实际多源学习工作流中的可扩展和自适应数据集选择，能够有效处理异构数据池中的数据集选择问题

Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.

</details>


### [74] [Bidirectional Normalizing Flow: From Data to Noise and Back](https://arxiv.org/abs/2512.10953)
*Yiyang Lu,Qiao Sun,Xianbang Wang,Zhicheng Jiang,Hanhong Zhao,Kaiming He*

Main category: cs.LG

TL;DR: BiFlow是一种双向归一化流框架，通过近似逆映射而非精确解析逆，解决了因果解码瓶颈，在ImageNet上实现了生成质量提升和采样速度两个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 传统归一化流需要精确解析逆变换，限制了架构灵活性。TARFlow等变体虽然结合了Transformer和自回归流，但暴露了因果解码的主要瓶颈，导致采样效率低下。

Method: 提出双向归一化流(BiFlow)框架，学习一个近似噪声到数据逆映射的反向模型，无需精确解析逆，从而支持更灵活的损失函数和架构设计。

Result: 在ImageNet上的实验表明，相比因果解码对应方法，BiFlow提高了生成质量，同时将采样速度加速了高达两个数量级，在基于NF的方法中达到最先进水平，在单评估方法中具有竞争力。

Conclusion: BiFlow通过移除精确解析逆的要求，解决了归一化流中的因果解码瓶颈，为这一经典范式提供了更灵活高效的框架，有望推动归一化流研究的进一步发展。

Abstract: Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [75] [Hyperspectral Image Data Reduction for Endmember Extraction](https://arxiv.org/abs/2512.10506)
*Tomohiko Mizutani*

Main category: eess.IV

TL;DR: 提出一种数据缩减的自字典方法，用于高光谱图像端元提取，显著降低计算时间而不损失精度


<details>
  <summary>Details</summary>
Motivation: 自字典方法虽然端元提取精度高，但计算成本大，限制了其在大规模高光谱图像中的应用。现有缓解方法仍面临挑战，因此本文采用数据缩减策略。

Method: 基于线性混合模型和纯像素假设，开发数据缩减技术去除不含端元的像素。理论分析表明该缩减步骤能保留接近端元的像素。在此基础上，提出数据缩减的自字典方法，将数据缩减与基于线性规划的自字典方法相结合。

Result: 数值实验证明，所提方法能显著减少原始自字典方法的计算时间，同时不牺牲端元提取精度。

Conclusion: 通过数据缩减技术，成功解决了自字典方法计算成本高的问题，实现了高效且准确的高光谱图像端元提取。

Abstract: Endmember extraction from hyperspectral images aims to identify the spectral signatures of materials present in a scene. Recent studies have shown that self-dictionary methods can achieve high extraction accuracy; however, their high computational cost limits their applicability to large-scale hyperspectral images. Although several approaches have been proposed to mitigate this issue, it remains a major challenge. Motivated by this situation, this paper pursues a data reduction approach. Assuming that the hyperspectral image follows the linear mixing model with the pure-pixel assumption, we develop a data reduction technique that removes pixels that do not contain endmembers. We analyze the theoretical properties of this reduction step and show that it preserves pixels that lie close to the endmembers. Building on this result, we propose a data-reduced self-dictionary method that integrates the data reduction with a self-dictionary method based on a linear programming formulation. Numerical experiments demonstrate that the proposed method can substantially reduce the computational time of the original self-dictionary method without sacrificing endmember extraction accuracy.

</details>


### [76] [Fast and Robust LRSD-based SAR/ISAR Imaging and Decomposition](https://arxiv.org/abs/2512.10740)
*Hamid Reza Hashempour,Majid Moradikia,Hamed Bastami,Ahmed Abdelhadi,Mojtaba Soltanalian*

Main category: eess.IV

TL;DR: 提出了一种快速统一的联合SAR成像框架，通过鲁棒的低秩稀疏分解处理平台残余相位误差，并显著降低计算负担。


<details>
  <summary>Details</summary>
Motivation: 现有低秩稀疏分解驱动的SAR成像方法在面对平台残余相位误差时性能不佳，且未考虑实时处理需求，计算负担重。

Method: 提出快速统一的联合SAR成像框架，通过鲁棒低秩稀疏分解增强稀疏目标和低秩背景特征，避免大矩阵求逆，利用约束二次规划处理单模约束。

Result: 在合成和真实数据实验中，该方法在成像质量和计算成本方面均优于现有先进方法。

Conclusion: 该方法有效解决了平台残余相位误差问题，显著降低了计算复杂度，适用于SAR和ISAR成像应用。

Abstract: The earlier works in the context of low-rank-sparse-decomposition (LRSD)-driven stationary synthetic aperture radar (SAR) imaging have shown significant improvement in the reconstruction-decomposition process. Neither of the proposed frameworks, however, can achieve satisfactory performance when facing a platform residual phase error (PRPE) arising from the instability of airborne platforms. More importantly, in spite of the significance of real-time processing requirements in remote sensing applications, these prior works have only focused on enhancing the quality of the formed image, not reducing the computational burden. To address these two concerns, this article presents a fast and unified joint SAR imaging framework where the dominant sparse objects and low-rank features of the image background are decomposed and enhanced through a robust LRSD. In particular, our unified algorithm circumvents the tedious task of computing the inverse of large matrices for image formation and takes advantage of the recent advances in constrained quadratic programming to handle the unimodular constraint imposed due to the PRPE. Furthermore, we extend our approach to ISAR autofocusing and imaging. Specifically, due to the intrinsic sparsity of ISAR images, the LRSD framework is essentially tasked with the recovery of a sparse image. Several experiments based on synthetic and real data are presented to validate the superiority of the proposed method in terms of imaging quality and computational cost compared to the state-of-the-art methods.

</details>
